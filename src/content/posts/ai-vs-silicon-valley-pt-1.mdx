---
title: 'AI vs Silicon Valley, part 1'
subtitle: "Exploring the AI problems"
pubDate: 2024-05-30
metaDescription: "Exploring the AI problems"
icon: 'folder_1'
tags: ['code', 'life']
private: false
---



"I've lost all faith in humanity." This is a phrase that I've seen being thrown around a lot. Whenever I see it used by grumpy middle-aged men with Ron Swanson profile pictures commenting on Reddit under a post about people camping outside an Apple store on the day a new iPhone dropped, I can't help but feel a bit annoyed. If this is all it takes to make you give up on humanity, wait till you hear about something called "war". Of course, I understand that whoever uses this phrase usually doesn't _really_ mean it. I remember that back in 2010, seeing a photo of young fans going crazy for Justin Bieber was enough for Ron-Swanson-profile-pic internet commentator to get all upset. Me, on the other hand, I try to stay positive, even when I witness such serious transgressions as people liking musicians that I don't. However, I must admit that in recent years my faith in humanity has been tested on many occasions, not in an exaggerated ironic way, but in a serious way. Even though I won't be talking about something as heavy as wars or human rights violations, the topic of this post is still pretty serious. I want to talk about one of the hottest new fads – something called "AI." Please stick with me till the end. You'll understand what faith in humanity has to do with it.

This post will be very critical of AI, so before I'm accused of being a Luddite ([not that there's anything wrong with being one](https://headgum.com/factually-with-adam-conover/why-big-tech-is-ruining-our-lives-with-brian-merchant)), I want to make it clear that I'm not _against_ AI. I think it is genuinely one of the most significant technological breakthroughs in recent human history. It has the potential to improve people's lives and maybe even revolutionize entire industries. In my opinion, it's not a solution to all the world's problems (as many "experts" are suggesting), but a _tool_ that can be useful _in specific contexts_. However, like any new technology, AI is marred by very serious problems. These problems must be adequately addressed, even if this comes at the cost of slowing down. As Subbarao Kambhampati, a professor in the School of Computing & AI at Arizona State University, had put it ["A clear-eyed understanding of the strengths and limitations of a technology is a step towards advancing it. Blind cheerleading or unalloyed cynicism, in contrast, are just steps towards advancing your influencer career.."](https://x.com/rao2z/status/1723008771557839204) Frankly, I believe that the antagonism between "progress" and "solving issues this progress brings" is often greatly exaggerated and used in bad faith. Both can be done at the same time. I'll leave it at this for now as I plan to explore this topic in part 2. Either way, AI's problems are severe, as will become apparent from this post, and should be addressed as quickly as possible. Unfortunately, this is extremely difficult due to the impenetrable bubble of hype and disinformation that surrounds this tech. Honestly, I think it's almost impossible for the average person to even understand how AI _actually_ works because so many news articles, social media posts, and even scientific papers on the topic are full of complete nonsense and outright lies. In truth, even the term "AI" itself is very misleading, as even the craziest, most devoted AI cultists don't really believe that whatever tech we have right now can be called a true "Artificial Intelligence" since even the most advanced LLMs are severely lacking in the "intelligence" department. Some of the least critical researchers talk about systems we have right now having only little glimpses of "intelligence" or ["sparks of AGI" (Artificial General Intelligence)](https://arxiv.org/abs/2303.12712) as they put it themselves. Meanwhile, other researchers even argue (very convincingly) that [AI isn't "artificial" nor is it truly "intelligent".](https://www.wired.com/story/researcher-says-ai-not-artificial-intelligent/) In fact, despite many claims from some industry experts and even some prominent scientists that AI is actually intelligent, or sentient or maybe even has a soul, there's overwhelming evidence indicating that has none of these things, and in truth, isn't all that "smart." This topic is vast, so I want to get too deep into it in this post. Either way, while the fact that the current AI is a "real AI" is still debatable, the dangers posed by this technology are undoubtedly very real. Many experts, researchers and organizations are fighting day and night to educate people about this technology and bring the public's and government's attention to these problems. Unfortunately, this fight against disinformation is still an uphill battle because so much of it is actually being pushed by the biggest multibillion-dollar corporations (such as Google and Microsoft) and the wealthiest, most powerful men in the world (such as Elon Musk and Bill Gates). They use their almost infinite resources and influence to shift the public's attention from real, tangible problems of the present to some very vague "AI potential" of the future. [AI hype in itself is a severe issue.](https://link.springer.com/article/10.1007/s43681-024-00461-2) If the real problems posed by AI aren't solved soon, and we as a society continue on the current trajectory, all of us might end up in very dark territory. With this post, I'm hoping to make a small contribution to this fight against disinformation. I think that it's absolutely fair to say that, at this moment, the negatives of AI in its current state outweigh entirely its positives, and something should be done about this. I hope you will agree with me by the end of this post.

In part 1 (this post), I will go through some of the most severe problems posed by AI technology in detail. For each of these problems, I will provide an analysis and my personal opinions. This article might seem one-sided, and that's on purpose – in my opinion, there are plenty of articles on the internet praising AI and not nearly enough critiquing it, so I'm hoping to bring balance to the overall picture. However, I don't think I'm treading any new grounds with this article; pretty much all the information and ideas you can find here are taken from somewhere else. My intention is to aggregate all this information in one place and have this post as a single point of reference for everything related to some of the dangers and problems AI creates. I also want it to be a good starting point for continuing research for anyone who wants to dive deeper into the topic. I tried to make this article as accessible as possible. Some things might require some technical knowledge, but it's not critical. However, if you'd like a primer on how AI actually works under the hood, I highly recommend [this video series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) and [this video on LLMs by Andrej Karpathy.](https://www.youtube.com/watch?v=zjkBMFhNj_g&pp=ygUQa2FycGF0aHkgYW5kcmVqIA%3D%3D)

In part 2, I will take a deep dive into the crazy polemic of the most prominent AI hype-men. I will explore those people's thought processes, philosophies, and goals. In the end, I will share my thoughts on the future of Silicon Valley and humanity's approach to technological advancement in general.

I have been writing this post for almost an entire year. I guess I shot myself in the foot trying to write a comprehensive article about a constantly changing and evolving topic. Frankly, this has been a very exhausting journey, so I don't know if part 2 will come out anytime soon, if ever at all. I don't want to deal with moving targets anymore, so I have decided to wait until this hype bubble bursts before working on part 2. Also, before we proceed any further, I want to note that I will be using the terms "artificial intelligence" and "AI" in a very broad sense – just as they have been used by pretty much everyone else these days, especially marketers. If a technology has something to do with machine learning – it's an AI. Hey, it's 2024, and words have no meaning anymore, so I can do whatever I want. Lastly, this post is still a work in progress, so if you find any mistakes or you have something to add to what's written here, please let me know.

Now, without any further ado...


### AI "sweatshops" in the Global South and around the world

At the heart of what most people currently call "AI" is the technology called ["Artificial Neural Network."](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) An Artificial Neural Network (usually called simply Neural Network or NN) is, roughly speaking, a program which design is _inspired_ by the anatomy of the human brain. Before this program can be used for anything, it needs to go through a process called "training" or "learning." You might've seen the term Machine Learning or ML – this is what this field was called before it was hijacked by marketing teams pushing for calling Neural Networks AI for the sake of hype. In fact, calling the process "training" or "learning" is also a bit misleading. "Function fitting" would've been more…uhm fitting here. Either way, this training process is integral to the development of almost all modern AI tech. Most powerful NNs, such as ChatGPT, Gemini (ex-Bard), Midjourney or SnapML, require LOTS and LOTS of data to be trained. This data needs to be obtained and processed somehow. I'll talk about issues with sourcing the data later. For now, I would like to focus on processing or preparing data for training. What does this usually entail? Well, if we are building an NN that's capable of detecting whether a photo has a bicycle, for example, we would need to:

* Make sure that each photo is the same size in pixels.
* Each photo can be read by the program without any issues and has the correct file format.
* Most importantly, each photo must have an arbitrary "yes" (meaning there's a bicycle in the photo) or "no" (no bicycle) label associated with it.
* Perhaps there are some other considerations that I haven't thought about.

Even though some of this work can be automated, the most crucial part – labelling the photos – must be manually performed by people. And this is what processing or preparing the data essentially means – sorting, ordering, filtering, resizing, and most importantly, labelling each and every individual piece of data used for training. Unfortunately, there's no magical solution to this. A real-life human has to sit down and sift through the photos, carefully marking them individually. Imagine going through the thousands of pictures like this. This is an incredibly difficult, stupefyingly tedious and, above all, very time-consuming process. How did the big corporations solve this problem? Pretty much like how they solved all their problems with the dangerous, expensive and equally as tedious industrial production processes – [they outsourced (most of) this work to the Global South.](https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/) TIME reported that [OpenAI used Kenyan workers making $2 an hour to filter traumatic content from ChatGPT.](https://time.com/6247678/openai-chatgpt-kenya-workers/) I will be loosely quoting the article in a paragraph below, please give original a read if you'd like to get a better picture.

In order to build an "AI" that could detect toxic language like hate speech and to help remove it from their platforms, OpenAI needed to train it on labelled examples of violence, hate speech, and sexual abuse. To that end, they engaged with Sama, a San Francisco-based firm that employs workers in Kenya, Uganda and India to label data for Silicon Valley clients like Google, Meta and Microsoft. Sama markets itself as an "ethical AI" company and claims to have helped lift more than [50,000](https://www.sama.com/blog/we-are-a-b-corp/) people out of poverty. However, despite their integral role in building ChatGPT, the workers at Sama faced gruelling conditions and low pay. An agent working nine-hour shifts could expect to take home at least $1.32 per hour after tax, rising to as high as $1.44 per hour if they exceeded all their targets. There is no universal minimum wage in Kenya, but at the time these workers were employed, the minimum wage for a receptionist in Nairobi was $1.52 per hour. To get the training data processed and labelled, OpenAI sent tens of thousands of text snippets to Sama beginning in November 2021. Much of that text appeared to have been pulled from the darkest recesses of the internet. Some of it described situations in graphic detail, like child sexual abuse, bestiality, murder, suicide, torture, self-harm, and incest. Needless to say, going through this data is a very hard ordeal. Some workers described being mentally scarred by the work. Some even went as far as calling it a torture. And, of course, OpenAI isn't the only company that worked with Sama. TIME also published another relevant story titled ["Inside Facebook's African Sweatshop."](https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/) The investigation detailed how Sama employed content moderators for Facebook, whose jobs involved viewing images and videos of executions, rape and child abuse for as little as $1.50 per hour.

Considering that OpenAI is [valued above $29 billion](https://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279), while Microsoft alone [has already invested in it a whopping $10 billion](https://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in-openai#xj4y7vzkg), it really does make you think – why can't such a wealthy company create better conditions for workers responsible for one of the most challenging and most essential parts of their core product?

Big corporations aren't the only ones trying to outsource this problem. Small startups also often do the same. Some get pretty creative with it, like [Metroc, a Finnish startup that is using prison labour to help it train AI.](https://www.wired.com/story/prisoners-training-ai-finland/) The reason they had to resort to such a solution is that not many people in the Global South speak Finnish, so they had to look for cheap, easily exploitable workers within their country's borders.

"The changes coming are unstoppable," [Sam Altman, the CEO of OpenAI, said.](https://www.cnbc.com/2021/03/17/openais-altman-ai-will-make-wealth-to-pay-all-adults-13500-a-year.html) "If we embrace them and plan for them, we can use them to create a much fairer, happier, and more prosperous society. The future can be almost unimaginably great." Knowing what the development process of OpenAI's products involves, I can't help but wonder: who is he talking about when he talks about "society"? Definitely not the people working on building this technology for him. In my opinion, the AI that is supposed to "create a much fairer, happier, and more prosperous society" can't require hundreds of people slaving away in sweatshop-like environments in order to work. What we have right now feels less like a technology of the future and more like a very impressive [Mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk) that, on the inside, is actually a primitive mechanism being operated by slave children pushing pedals. With such incredible amounts of money thrown at this thing, we can do so much better.

In fact, the comparison to Mechanical Turk is much closer to the truth than it might seem at first. Some of the AI projects don't even use AI at all. [Fortune calls this "AI-washing"](https://fortune.com/2024/03/18/ai-washing-sec-charges-companies-false-misleading-statments/) – AI is so hot right now that companies are just too eager to claim that their products use AI, even if, in truth, they have nothing to do with machine learning. They write that some companies were charged by the SEC for making such false claims. [And often what hides behind the "Artificial Intelligence" isn't even software, but good old human labor.](https://www.forbes.com/sites/cognitiveworld/2020/04/04/artificial-or-human-intelligence-companies-faking-ai/?sh=71395b39664f) More prominent examples include [Presto Automation,](https://www.theverge.com/2023/12/8/23993427/artificial-intelligence-presto-automation-fast-food-drive-thru-philippines-workers) a company which AI fast food drive-thru tech turned out to be mostly just human workers in Philippines, and Kaedim, [an AI startup that allegedly has been using cheap human labor for "generating" 3D models.](https://www.404media.co/kaedim-ai-startup-2d-to-3d-used-cheap-human-labor/) Amazon got into hot water recently, because of the claims that their "Just Walk Out" system – a system that tracks customers as they walk around the store, detects what they take, and then automatically charges them as they leave the store with those products without them having to use a checkout – [this system was allegedly using 1000 human workers from India instead of AI.](https://gizmodo.com/amazon-reportedly-ditches-just-walk-out-grocery-stores-1851381116) [However, Amazon later clarified, that Indian workers they employ are only "tasked with annotating AI-generated and real shopping data to improve the Just Walk Out system — not run the whole thing."](https://www.theverge.com/2024/4/17/24133029/amazon-just-walk-out-cashierless-ai-india) Even so, from all this we can draw the conclusion that the one of the most important "secret ingredients" of almost any hyped up AI system is underpaid workers from the Global South. Even when it isn't fake, whether it's fine-tuning a model, setting up guardrails and system prompts, or preparing a dataset, AI always requires lots and lots of hidden human labour.


### AI generated content is ethically and legally dubious

Unfortunately, the process of obtaining data required for training NNs is also often problematic. Let's take a look at "generative AIs," for example. A "generative AI" is a sort of AI that can generate different types of content (such as images, videos, music, etc) after being trained on many examples of this kind of content. For example, in order to produce an image that looks like an artistic drawing, a generative AI has to be trained on millions or even billions of real artists' works. In order to obtain those works, the company working on AI has to [scrape (scraping is the process of extracting data from websites)](https://en.wikipedia.org/wiki/Web_scraping) the web downloading images from resources such as DeviantArt and ArtStation, and feed them to the NN for training. The issue with this is that no one actually bothered to ask artists whether they want their works to be used in such a way. Another problem is that the legality of this process is also very dubious, considering that a lot of art used for training is protected under Copyright. When asked by [Forbes](https://www.forbes.com/sites/robsalkowitz/2022/09/16/midjourney-founder-david-holz-on-the-impact-of-ai-on-art-imagination-and-the-creative-economy/?sh=33dbd4ca2d2b) in a recent interview if the Midjourney seeks out permission from the artists whose images they've used to train their program, company's CEO David Holz simply said there was no feasible way to do so. "No," Holz underscored. "There isn't really a way to get a hundred million images and know where they're coming from. It would be cool if images had metadata embedded in them about the copyright owner or something. But that's not a thing; there's not a registry. There's no way to find a picture on the Internet and then automatically trace it to an owner and then have any way of doing anything to authenticate it."
Other companies have made similar statements. ["Because copyright today covers virtually every sort of human expression – including blog posts, photographs, forum posts, scraps of software code, and government documents – it would be impossible to train today's leading AI models without using copyrighted materials," said OpenAI in its submission, first reported by the Telegraph.](https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai)
In my opinion, if you decide to become obscenely rich through means that are both unethical and illegal, you should probably reconsider. For example, if there's no ethical and legal way to steal other people's money from a bank, you shouldn't do it. Even if stealing money from a bank was somehow legal, an ethical problem would remain, as stealing other people's money hurts them directly. Does this make sense? I think the same logic applies here, and we should find a better way to go about things. But maybe some special Silicon Valley tech bro logic is involved in this that I don't yet comprehend? Anyhow, this situation has created, as Kotaku had written, ["an ethical and copyright black hole, where everyone from artists to lawyers to engineers has very strong opinions on what this all means, for their jobs and for the nature of art itself."](https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060) In my opinion, there can be no debate on the ethics of this approach – using other people's work without their consent is bad, no matter what angle you look at it from. The legality of this is another issue, however. Naturally, after seeing how their works have been used by AI without their consent to generate profit for 3rd parties, many people went to court to fight for their rights. [Stability AI, Midjourney, and DeviantArt are all currently being sued by artists for copyright infringement](https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart). The AI corporations/start-ups are trying to stall the lawsuits in any way they can. Thankfully, their attempts haven't been very successful so far. For example, relatively recent ( and, in my opinion, nonsensical) [arguments from Midjourney's lawyers that "the suit is intended to suppress its free speech" have been dismissed by the court.](https://www.hollywoodreporter.com/business/business-news/artist-lawsuit-ai-midjourney-art-1235821096/) This kind of rhetoric looks desperate, if you ask me. But why the desperation? Well, the first reason is, as Axios puts it, that ["looming \[court\] fights over copyright in AI are likely to set the new technology's course in 2024 faster than legislation or regulation."](https://www.axios.com/2024/01/02/copyright-law-violation-artificial-intelligence-courts) The representatives of the top AI companies and "leading experts" have been doing all in their power to sweet-talk the governments to not regulate AI. How they do it is something I will talk about later, but the answer to the question "why" is that the regulations will hurt corporate profits, obviously. Because of lobbying and other kinds of manipulations, many of the world's governments have been very slow to address the dangers posed by AI in any meaningful way, while these dangers have been growing more significant at an increasing pace. However, a court decision might be able to rein in AI before governments have a chance to do so, and this is something AI companies want to avoid. And so far, it doesn't look like AI companies have a good chance of winning their cases. Let's take a look at probably the most important lawsuit of them all – OpenAI v New York Times. Ars Technica has an [excellent article](https://arstechnica.com/tech-policy/2024/02/why-the-new-york-times-might-win-its-copyright-lawsuit-against-openai/) exploring this lawsuit. In it, the authors go through the history of similar cases and outline their decisive factors. Often, the companies accused of copyright breaches try to claim fair use, as does OpenAI. The article states that the "courts are supposed to consider four factors in fair use cases, but two of these factors tend to be the most important. One is the nature of the use. A use is more likely to be fair if it is "transformative"—that is if the new use has a dramatically different purpose and character from the original ... The other key factor is how a use impacts the market for the original work." The article analyzes the Authors Guild v. Google lawsuit in particular. Back in 2004, Google announced its new project – Google Books. The idea was to scan millions of books and make them available in their search engine. Authors and publishers argued that doing so was violating copyright laws and took their argument to court. However, Google was able to win the lawsuit primarily because, first, the use of the original material was transformative – the users were able to search for text from a particular book using Google Search, but they were unable to read the book in its entirety using this service – and second, because it was having a positive impact on the book market – with the help of Google Books people were able to find new books that interested them. Imagine doing a research. You type in the information that you're looking for in Google's search bar, and at the top of the search results, you see the Google Books webpage. You can view a few pages of the book on it and see whether it has something you're looking for or not. If you find this book useful, you will buy it, naturally. In the end, this is a win-win-win situation – it's a win for the publisher/author, a win for the user and a win for Google. Now let's take a look at ChatGPT; how does it fare in regard to the 2 aforementioned fair use factors? First, the ChatGPT's use isn't transformative. NYT has compiled a very strong case; they provided more than 100 examples, demonstrating that ChatGPT often quotes NYT articles verbatim, several paragraphs at a time. Even though OpenAI claims that this is just a bug and something that they are working on eliminating, it is clear that their position is very shaky here. Even if they will be able to eliminate this bug completely via fine-tuning, the problem here is that this demonstrates that the system is _capable_ of regurgitating copyrighted content. In the lawsuit documents, we can see that ChatGPT outputs copyrighted data when prompted to do so directly. However, there's also a chance that an AI can output big chunks of copyrighted content during a regular conversation without citing the source. LLMs actually struggle a lot with providing the sources of the information they were trained on. As this blog post from Microsoft states, ["ChatGPT will make attempts to provide sources for its content, but its primary function is to reproduce patterns in text, not to actively consult sources to provide accurate information."](https://www.microsoft.com/en-us/microsoft-365-life-hacks/writing/using-chatgpt-for-source-citation) Also, fun fact: AI reproduces creative works as part of the training process. OpenAI has noted this in [court](https://storage.courtlistener.com/recap/gov.uscourts.cand.415174/gov.uscourts.cand.415174.32.0.pdf) and other official filings. The bigger problem here, however, and this also ties in with the second factor, is that ChatGPT will most likely actually compete with NYT, as well as many other news outlets. [Microsoft has plugged ChatGPT into its Bing search,](https://www.microsoft.com/en-us/bing?ep=278&form=MA13LT&es=31) which signals to us that one of the potential use cases the company sees for the AI systems is to be a go-to single all-encompassing source of information. In other words, they want AI to replace the Internet. Instead of searching the web, the users would prompt AI and get answers without navigating away from the Bing page. Microsoft isn't the only one that has come up with this idea. The Browser Company (a company behind Arc browser) was quick to jump on the AI hype train and has released an [Arc Search app.](https://arc.net/blog/arc-search) This app is "focused on getting you to an answer faster, without all the clutter of the internet." Its promise is that it "browses for you" – you type in your question, and it gives you an AI-generated answer. This kind of thing is problematic from multiple angles, but what's important for now is that this app competes with websites it takes its information from. Google [has similar plans](https://search.google/features/search-labs/) for its Bard/Gemini/Whatever-it-will-be-called-next AI, and there are a few other examples. How do these systems compete with NYT? Now, if people use the Bing search engine, they might get NYT as a first search suggestion, find an answer there, and bring in some revenue via ad impressions/clicks or maybe even potentially become their customer (by purchasing a subscription, for example). With the AI system being pushed as a replacement for search, the users will get the information they are looking for right in the AI chat box, and the chances that they will navigate to the NYT website from there are very slim. I mean, why open the original article to read the same information you just read in the chat box? By substituting the Internet with AI, you substitute NYT.com with AI. I think this is obvious. Actually, legal and ethical problems aside, this kind of system is unsustainable, in my opinion. Imagine you own a small, very specialized website containing valuable information on a certain topic. This information needs to be updated regularly, so to keep the whole thing running, you make some income from ads, + you have a link to your Patreon (or other monetization platform) on your website. Imagine an AI company that also regularly scrapes all the information from your website and uses it to train its model. Then, let's imagine that this company would have an AI that is able to provide the same information you have on your website soon after you post it. If an AI system is injected into people's information search flow as the first and only step, as is the intention with Bing AI, people would be finding the information you provide via an AI and considering how it struggles with citations, they would never discover your website. Naturally, because of this, the income you were getting to support your project would start to dwindle. You might wrap up your project or change the format of your content delivery (start publishing it in a magazine, for example), and because of this, the value of AI for people who were using it to get the info you were providing would fall. As AI hurts organizations and individuals by extracting the information they provide and then presenting it as its own, these organizations or individuals would either go out of business or try to become unreachable by scraping. Either way, by trying to replace the Internet, AI companies might shoot themselves in the foot. They are aware of this and use various approaches to deal with it. Google, for example, has decided to resort to an unthinkable solution – it plans to strike a deal with organizations whose data it'll use to train their AI and actually pay them! Well, there's only one such organization for now – [Google's made a deal with Reddit to use their data to train AI.](https://www.theverge.com/2024/2/22/24080165/google-reddit-ai-training-data) The Reddit users, who are actually the ones producing content for the platform and thus bringing any value to the company, weren't asked. Anyway, this [isn't the first unpopular decision Reddit's leadership has made.](https://www.techdirt.com/2023/06/16/reddit-ceo-triples-down-insults-protesters-whines-about-not-making-enough-money-from-reddit-users/) OpenAI, on the other hand, has been scraping the content that is supposed to be unscrapable. Or at least, it seems that way, as there has been [evidence that their web crawler ignores Robots.txt.](https://news.ycombinator.com/item?id=40001971) Getting back to the NYT lawsuit, even though they had prepared a very strong case, it's still impossible to predict which way the events will go. Either way, I will be monitoring this situation, and I recommend you do the same. Probably, the second most important lawsuit that has the potential to define the future of technology is Getty v Stability AI, so I recommend watching this one closely as well.

Here, I want to make a very important note. Copyright is a very nuanced topic. Even if some valid arguments can be made against it, it cannot be simply abolished overnight. Some of the AI supporters holding right-wing libertarian political views have been touting AI as a tool that could potentially dismantle the copyright system. The problem is that if we want to avoid a catastrophe, there needs to be an alternative system in place before any dismantling can happen. Otherwise, this would be a disaster, as entire industries and millions of people would lose the means to support their livelihoods. As Dominic Young, the CEO of Axate, writes in his [report for the House of Lords Communications and Digital Select Committee](https://committees.parliament.uk/writtenevidence/128368/pdf/), "Copyright is a precious & powerful economic & cultural engine. When it's undermined, as it was for the benefit of the big platforms…it destroys value & creates monopolies. We must not repeat that historic mistake when it comes to AI."

Meanwhile, some companies like Adobe managed to sidestep the legal issues altogether, at least for now. But even well-established mega-corporations have to take a few "ethical shortcuts" to train a generative AI capable of competing with Midjourney and OpenAI's DALL-E. Despite Adobe boasting that their ["Firefly AI"](https://www.creativebloq.com/features/everything-you-need-to-know-about-adobe-firefly) is the most "ethical" generative AI, people are very [skeptical](https://newart.press/p/is-adobe-fireflys-ethical-dataset) especially considering Adobe's [very unethical track record](https://en.wikipedia.org/wiki/Adobe_Inc.#Criticisms). Firefly's datasets are sourced primarily from Public Domain resources as well as their own service – Adobe Stock. Basically, Adobe is leveraging its monopolistic position in the graphic design market to obtain 100% legal images for training their model. "100% legal" doesn't mean explicit consent, however. Many artists don't even know their works are being used to train an AI. Of course, whenever someone submits their work to Adobe Stock, they give Adobe legal rights to do anything the company wants with it, including AI training. It's right there, hidden away in [Contributor Agreement Additional Terms](https://wwwimages2.adobe.com/content/dam/cc/en/legal/servicetou/Adobe_Stock_Contributor_Agreement_Addl_Terms_en_US_20220415.pdf) as "developing new features and services" – the definition so vague it's pretty much the equivalent of fine print. And of course, as of now, there's [no way to opt out](https://helpx.adobe.com/ca/stock/contributor/help/firefly-faq-for-adobe-stock-contributors.html) of data set training for content submitted to Stock. However, in April 2024, it became known that Adobe has been [actually using Midjoureny-produced images to train Firefly.](https://www.bloomberg.com/news/articles/2024-04-12/adobe-s-ai-firefly-used-ai-generated-images-from-rivals-for-training) I think at this point, we can definitely say that Firefly AI is anything but "ethical."

Something that I think is very important to mention here is that there's an opinion that the most important thing about any AI model is its dataset. In other words, AI models are their dataset. [OpenAI engineer James Betker writes](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/) that "...trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point... [This] implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It's determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.
Then, when you refer to "Lambda," "ChatGPT," "Bard,"" or "Claude," then it's not the model weights that you are referring to. It's the dataset." This observation is very interesting and makes a lot of sense. However, this fact has some very serious implications for how AI cannot be a copyright infringement, considering models are their dataset.

Ok, the use of copyrighted work by AI is problematic, but what is the legal status of the works produced by an AI? [United States District Court has ruled that no work produced by AI can be copyrighted.](https://www.theverge.com/2023/8/19/23838458/ai-generated-art-no-copyright-district-court) In a way, this implies that you can't make much of money being just "an-AI artist." Unless you make a significant part of the work yourself, your AI-produced content can't be considered as something that you made yourself. As it should be, in my opinion. This makes AI seem more like just one more tool in artists' repertoire and less like an all-encompassing complete solution to art and a total replacement for artists. In fact, if this technology was presented in this more honest way, and was developed ethically, the reception by the artists would've been completely different. But you see, "just a tool" doesn't fit into the disruption-unicorn narrative many AI companies are trying to spin for themselves. Unfortunately, greed and delusion drive today's tech market, but more on this later.


### AI has privacy issues

As I mentioned above, AI models contain their training dataset (in a compressed form, more on that later) within themselves and are capable of reproducing the training data almost word for word, pixel for pixel. Now, what would happen if your private information would somehow end up in the training dataset? Well, this means that there's a good chance that this data can be accessed by almost anyone using the AI, given they can figure out the correct prompt to access it. An Indiana University Bloomington Ph.D. candidate Rui Zhu claimed that he acquired private email addresses of more than 30 New York Times employees using OpenAI's ChatGPT, [The NYT reports.](https://www.nytimes.com/interactive/2023/12/22/technology/openai-chatgpt-privacy-exploit.html) The experiment showed users can "bypass the model's restrictions on responding to privacy-related queries," he said. In fact, [private data extraction can be performed at scale, as Google researchers had shown.](https://techxplore.com/news/2023-12-prompts-chatgpt-leak-private.html) They found they could utilize keywords to trick ChatGPT into tapping into and releasing training data not intended for disclosure. In a blog post on their findings, Google researchers said, "OpenAI has said that a hundred million people use ChatGPT weekly. And so, probably over a billion people-hours have interacted with the model. And, as far as we can tell, no one has ever noticed that ChatGPT emits training data with such high frequency until this paper." Also, anything you share with it is retained and used to further train the model unless you explicitly opt-out. So, even if your private data isn't initially in the training dataset, it might still end up exposed if you ever share it with an AI. [Samsung workers ended up in big trouble after it was found out that they leaked trade secrets via ChatGPT.](https://mashable.com/article/samsung-chatgpt-leak-details) They shared private information with the LLM without a second thought, and eventually, this information appeared in the AI's responses and was available to anyone. **You should always avoid sharing any private information with AI.** These problems aren't specific to ChatGPT, by the way, as [other systems can also leak confidential data.](https://www.platformer.news/amazons-q-has-severe-hallucinations/)

What makes the whole situation worse is the fact that [once certain information ends up in the model's "memory," there's pretty much no way to delete this information without re-training the model.](https://www.theregister.com/2019/07/15/ai_delete_data/) In other words, in order to make an AI forget a piece of information, it needs to be re-trained on its training dataset without this piece of information. Considering that training is a process that takes months and costs many millions of dollars, no one is going to bother with deleting your private info if you ever end up sharing it with a chatbot. There's been some [research into making AI "unlearn" things](https://hbswk.hbs.edu/item/qa-seth-neel-on-machine-unlearning-and-the-right-to-be-forgotten), but I'm unaware if there's been any success so far. Even if the developers can tweak AI via prompt engineering or fine-tuning so that it doesn't expose the information easily, it's still at risk. Unlike regular secure data storage, even a random prompt might end up the one that can break the AI and make it spew out private information.

All this also means that AI might not meet GDPR requirements as it doesn't guarantee [the right to be forgotten.](https://gdpr.eu/right-to-be-forgotten/)


### Creative workers are being replaced with AI

While using artists' works without their permission is a severe problem in itself, it leads to other problems, some of which are even more serious. Many artists who already struggle to make ends meet feel that their livelihoods are being threatened by AIs that, as every "tech genius" boasts, are supposed to replace them. Here's the future that multibillion-dollar corporations envision for us: instead of paying an artist to commission a work from them, people are supposed to be paying the tech corporations so that they can use an AI to produce content generated from that artist's old works, all without paying the artist themself or even acknowledging them. [AI is already taking video game illustrators' jobs in China](https://restofworld.org/2023/ai-image-china-video-game-layoffs/), and [studios from other parts of the world are following suit.](https://kotaku.com/ai-art-layoff-video-game-studio-pc-midjourney-aigc-1850489333) Disney has [created a "task force to explore AI and cut costs."](https://www.reuters.com/technology/disney-creates-task-force-explore-ai-cut-costs-sources-2023-08-08/) The AI adoption by [game developers ("31% of game developers already use generative AI")](https://www.pcgamer.com/31-of-game-developers-already-use-generative-ai/), [movie companies](https://www.forbes.com/sites/neilsahota/2024/03/08/the-ai-takeover-in-cinema-how-movie-studios-use-artificial-intelligence/?sh=1ab4d924a3fd) and other creative studios has been very rapid and extensive. Returning back to Disney for a second here, the mega-corporation has already [used the technology in the production of season 2 of Loki TV show](https://www.theverge.com/2023/10/9/23909529/disney-marvel-loki-generative-ai-poster-backlash-season-2) and [in the opening credits of Secret Invasion TV show.](https://mashable.com/article/secret-invasion-ai-opening-credits) Considering all this, I think it's safe to say that the artists' fears are not unfounded.

Midjourney, as well as some other tools, even allow [using artists' names](https://decentralizedcreator.com/list-of-artists-supported-by-midjourney/) as prompts to generate content in their style. The list of supported artists' names is relatively short at the time of this post, but it is supposed to be expanded in the future.

According to the [SoA survey](https://www2.societyofauthors.org/2024/04/11/soa-survey-reveals-a-third-of-translators-and-quarter-of-illustrators-losing-work-to-ai/), about a quarter of illustrators (26%) have already lost their jobs due to AI, and over a third of them (37%) had reported that income from their work has decreased in value because of generative AI.

The AI replacement doesn't only apply to visual arts, by the way. Today, voice actors sitting down to record for games companies are forced to begin each session with "My name is **\_\_**, and I hereby grant irrevocable permission to train an AI with my voice and use it any way you see fit." Some game developers are already completely replacing voice actors with AI. For example, [Embark Studios, the developers of one of the biggest upcoming online FPS games, "The Finals," have been relying entirely on AI for its voicework.](https://www.forbes.com/sites/paultassi/2023/10/29/the-finals-is-using-ai-voicework-and-it-is-bad/?sh=54c98b491d34) Many voice actors and advocacy organizations, [report](https://www.vice.com/en/article/5d37za/voice-actors-sign-away-rights-to-artificial-intelligence) that contracts including language around synthetic voices are now becoming more prevalent. [AI-generated hip-hop has been flooding the internet](https://rollingstoneindia.com/just-because-ai-generated-rap-songs-go-viral-doesnt-mean-theyre-good/), and smaller artists are worried that there's yet another obstacle on their path to success.

Hollywood screenwriters' jobs have also been [under attack from AI.](https://www.wired.co.uk/article/hollywood-screenwriters-artificial-intelligence-guardrails) Even film actors aren't safe from AI. [Hollywood studios want to scan background actors for one day's pay, and then use likenesses for 'rest of eternity,' SAG claims.](https://people.com/sag-claims-studios-want-to-scan-background-actors-use-likenesses-for-free-7561285)

Considering all this, I think it's fair to say that there's a concerning trend in the creative industry to replace real people with AI. AI doesn't care, but people have to suffer terrible consequences, especially those already in a precarious position. To be completely honest, I find this situation sickening. There once was hope that most difficult, boring and repetitive tasks would be taken on by machines so that we, the humans, would have more free time to spend writing poems, painting, and composing music – in other words, creating art and expressing ourselves. Doing what we love, doing what makes us human. The promise of technology was to free us from hard and mundane labour and allow us to fulfill our true potential. I think there's some very dark irony in the fact that creative professions are among the first to come under attack by AI companies. I don't believe that this attack will be successful, partially because art is not something that can be "solved" by AI and because people trying to replace artists with AI misunderstand what art is at the fundamental level, but also partially due to the reasons I will get into later.

David Holz (CEO of Midjourney) [says that](https://www.forbes.com/sites/robsalkowitz/2022/09/16/midjourney-founder-david-holz-on-the-impact-of-ai-on-art-imagination-and-the-creative-economy/?sh=33dbd4ca2d2b) "Midjourney is designed to unlock the creativity of ordinary people by giving them tools to make beautiful pictures just by describing them." This means that before AI, someone who struggles with drawing could use _words_ to describe to an artist what they want to be drawn, pay the artist money for it, and then get an original (to a certain extent) work created by a human, who probably enjoyed the process of creating the work, and maybe even poured their heart and soul into it. However, what GenAI companies like Midjourney suggest is that with their technology, a person who struggles with drawing could use _words_ to describe what they want to be drawn to an AI and pay a corporation that owns it to get a tasteless amalgamation of regurgitated works used without their creators' permissions. I don't know about you, but this proposition seems worse to me. And I think it goes without saying that feeding someone's ideas into AI prompt text input has little to do with "unlocking creativity."

All in all, I think there might be a huge correction in the creative industry away from AI sometime in the future. The tech will definitely stay, but its usage will settle in a less prominent capacity than what AI hype-men preach. However, this won't come painlessly, as damage is already being done. Unfortunately, creative professions aren't the only ones being replaced with AI.


### ALL workers are being replaced with AI

It seems that workers in almost all professions are also at risk of being replaced by AI. In March of 2023, Goldman Sachs published a [report showing that AI could replace the equivalent of 300 million full-time jobs.](https://www.bbc.com/news/technology-65102150) In December 2023 [CNBC wrote an article stating](https://www.cnbc.com/2023/12/16/ai-job-losses-are-rising-but-the-numbers-dont-tell-the-full-story.html) that "more than one-third (37%) of business leaders say AI replaced workers in 2023, according to a recent report from ResumeBuilder." This report also states, "44% of business leaders report that there will be layoffs in 2024 resulting from AI efficiency." [About 10% of US workers are in jobs that face the greatest risk of disruption from rapidly evolving artificial intelligence, according to a White House analysis from March 2024.](https://www.cnn.com/2024/03/21/business/ai-jobs-white-house-work-force/index.html) Meanwhile, an [earlier report by PEW Research Center](https://www.pewresearch.org/social-trends/2023/07/26/which-u-s-workers-are-more-exposed-to-ai-on-their-jobs/) estimates that actually "19% of American workers were in jobs that are the most exposed to AI." [Business Insider has compiled a list of professions](https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02) that are most likely to be replaced by AI according to experts. Here's the list:

1. Tech jobs (Coders, computer programmers, software engineers, data analysts)
2. Media jobs (advertising, content creation, technical writing, journalism)
3. Legal industry jobs (paralegals, legal assistants)
4. Market research analysts
5. Teachers
6. Finance jobs (Financial analysts, personal financial advisors)
7. Traders
8. Graphic designers
9. Accountants
10. Customer service agents

The same SoA report I mentioned above also states that ["over a third of translators (36%) have already lost work due to generative AI."](https://www2.societyofauthors.org/2024/04/11/soa-survey-reveals-a-third-of-translators-and-quarter-of-illustrators-losing-work-to-ai/)

Many AI companies are already working overtime to fulfill these predictions. For example, Cognition Labs has been developing [Devin](https://www.cognition-labs.com/introducing-devin), an AI that is supposed to replace software engineers. [It has been advertised to be capable of completing jobs from Upwork from start to finish.](https://www.youtube.com/watch?v=UTS2Hz96HYQ) Meanwhile, Google has been aiming for journalists' jobs. It launched a ["private program for a handful of independent publishers last month, providing the news organizations with beta access to an unreleased generative artificial intelligence platform in exchange for receiving analytics and feedback," according to ADWEEK.](https://www.adweek.com/media/google-paying-publishers-unreleased-gen-ai/)

On the other hand, many business owners are quick to buy into the hype and replace their employees with AI systems. [Many companies are replacing their customer service teams with AI either partially or completely.](https://www.washingtonpost.com/technology/2023/10/03/ai-customer-service-jobs/) Some companies, like [BlueFocus, are replacing their copyrighters and creative writers with AI](https://www.campaignasia.com/article/embracing-ai-generated-content-bluefocus-will-replace-human-copywriters-and-crea/484033). Another example, [National Eating Disorders Association (NEDA) helpline which fired their entire staff and had transitioned to a chatbot, after workers tried to unionize.](https://www.vice.com/en/article/n7ezkm/eating-disorder-helpline-fires-staff-transitions-to-chatbot-after-unionization?callback=in&code=Y2IZZDZKM2YTNWZKMS0ZNGU1LWEYZJQTOGYZZTKYYJIXZGUW&state=736aaff608454002a718bb953eb39219) Cases like these are numerous.

For some people, [AI chatbots have been replacing real human therapists.](https://www.theguardian.com/lifeandstyle/2024/mar/02/can-ai-chatbot-therapists-do-better-than-the-real-thing) For some, it has been replacing [professional knowledge resources such as Stack Overflow.](https://meta.stackoverflow.com/questions/422392/chatgpt-seems-to-be-better-than-stack-overflow-both-in-speed-and-accuracy-what) Either through business decisions or changing public habits, a lot of people have been losing their jobs to AI.

Obviously, workers aren't too optimistic about the prospects of losing their means of subsistence. In fact, [according to a recent survey commissioned by Microsoft](https://www.microsoft.com/en-us/worklab/work-trend-index/will-ai-fix-work?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA&epi=TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA&irgwc=1&OCID=AIDcmm549zy227_aff_7593_1243925&tduid=%28ir__dvbthnpvjskfdj0ibyfo21pade2xevkyoul0jhle00%29%287593%29%281243925%29%28TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA%29%28%29&irclickid=_dvbthnpvjskfdj0ibyfo21pade2xevkyoul0jhle00), 49% of employees fear they will lose their jobs to AI. Workers already have to deal with an economic slowdown, inflation, [lower quality working conditions](https://www.ilo.org/global/about-the-ilo/newsroom/news/WCMS_865256/lang--en/index.htm), and now [AI anxiety](https://www.bbc.com/worklife/article/20230418-ai-anxiety-artificial-intelligence-replace-jobs) is added to the list of the problems. As if life wasn't stressful enough already!

However, replacing workers with AI doesn't only hurt the workers, it also hurts businesses because...


### AI is overhyped and unreliable


I’ll go straight to the point – in most cases, AI is actually **bad** for business. Why? Two main reasons: it **isn’t as powerful and capable as advertised**, and it’s also **extremely unreliable.** Many companies lured by promises of increased productivity from AI have been using it to replace employees. They are integrating it deep into their work processes, only to discover that the promised improvements have been greatly exaggerated. Why exactly is AI so unreliable? There are many factors, but I would like to outline 3 that, in my opinion, play the most significant role:

1. AI is often biased. Detecting and fixing a bias in AI is extremely difficult.
2. AI can hallucinate.
3. Lastly, AI has no real comprehension of the world or what it’s doing. [It can’t reason or plan.](https://cacm.acm.org/blogcacm/can-llms-really-reason-and-plan/) Google says AI is actually just [“computational statistics”](https://www.intgovforum.org/en/content/igf-2023-–-day-1-–-high-level-panel-v-artificial-intelligence-–-raw), and Meta/Facebook calls AI [“a giant autocomplete system.”](https://about.fb.com/news/2024/01/davos-ai-discussions/) Perhaps another good way of thinking about AI is as a [“Blurry JPEG of the web.”](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) After all, multiple studies (like [this one](https://arxiv.org/abs/1503.02406), or [this one](https://arxiv.org/abs/2309.10668), or probably most importantly [this one](https://arxiv.org/abs/2311.13110)) point out that lossy compression is pretty much all there is too deep learning. Put in most reductionist terms, “AI is a statistical model created via lossy compression; all it does is perform approximate retrieval.” Either way, As I mentioned before, I won’t be getting too deep into this topic here. Instead, I want to go through the other 2 factors.


### AI can be biased

You might ask, how can AI possibly be biased? Machines are supposed to be emotionless and rational! As we know from sci-fi media, AI is supposed to be cold, calculating and always objective in its judgement. Well, AI is trained on data that is produced, assembled and prepared by us humans, and we are far from being objective and unbiased. I'll explain. Back in 2016, Microsoft revealed their bleeding-edge chatbot named Tai AI. It was a simple Twitter bot that could carry conversations and use user responses to re-train and improve the model. Soon after the bot's launch, things went south very quickly. The problem was that it had only very basic filters (or perhaps even no filters at all) for what kind of replies it used for re-training. As Verge [have written about this](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist), "pretty soon after Tay launched, people started tweeting the bot with all sorts of misogynistic, racist, and Donald Trumpist remarks. And Tay — being essentially a robot parrot with an internet connection — started repeating these sentiments back to users, proving correct that old programming adage: flaming garbage pile in, flaming garbage pile out." It only took 24 hours for the bot to become a full-blown nazi. Soon after Tay's replies had gotten too spicy, Microsoft pulled the plug on AI and deleted all controversial tweets. Of course, modern AI chatbots have certain mechanisms that help them deal with blatant hate speech. For example, as we have learned, corporations contract entire companies to help them develop a model that would filter out harmful content at the training stage. However, while identifying and weeding out explicit hate speech is relatively easy, getting rid of hidden biases is a much more difficult task.

Pearson is one of the biggest and most respectable publishers of educational materials in the US. However, even their books aren't free from biases. In 2017, it was found out that their nursing textbooks contained racist material. [In a section on pain, the textbook titled "Nursing: A Concept-Based Approach to Learning" offered the following guidance:](https://www.insidehighered.com/news/2017/10/23/nursing-textbook-pulled-over-stereotypes)

- "Hispanics may believe that pain is a form of punishment and that suffering must be endured if they are to enter heaven."
- "Jews may be vocal and demanding of assistance."
- "Native Americans may prefer to receive medications that have been blessed by a tribal shaman."
- "Blacks often report higher pain intensity than other cultures."
- "Indians who follow Hindu practices believe that pain must be endured in preparation for a better life in the next cycle."
  
The note about black people is particularly harmful. Nurses who took this to heart might administer lower dosages of anesthesia for black people. It's mind-boggling that this kind of biased information persisted in educational materials till 2017. Now imagine if we had an AI trained on Pearson's educational materials! What if someone were to use that AI for medical advice? They would probably get accurate information in most cases, but in some, the AI's answer could be wrong or biased. When it comes to health, we can't afford to take any chances. Unfortunately, AI has penetrated many industries, including healthcare, and, as you've probably already guessed, has created many new problems due to its biases. For example, research has shown that [using AI to help with dermatological diagnosis exacerbates racial bias, even if overall accuracy improves.](https://news.northwestern.edu/stories/2024/02/new-study-suggests-racial-bias-exists-in-photo-based-diagnosis-despite-assistance-from-fair-ai/) In fact, biases can take many forms, each is equally problematic. One form of bias is sometimes called a ["learning shortcut" – when "spurious features (such as chest tubes and portable radiographic markers on intensive care unit chest radiography) on medical images are used for prediction instead of identifying true pathology."](https://www.sciencedirect.com/science/article/abs/pii/S1546144023005264) In other words, if, for example, we are training a model that we want to be able to diagnose pneumonia from a patient's x-ray, and in our training dataset, all images for patients that have pneumonia have a particular bone density, the model might erroneously associate that bone density with having pneumonia. In case of this form of bias, [according to a study from the University of Michigan "clinicians aren't able to catch when an AI is providing bad advice, even when given an explanation of the AI model's logic."](https://news.engin.umich.edu/2023/12/clinicians-could-be-fooled-by-biased-ai-despite-explanations/) So even being well aware of AI's biases doesn't really help to alleviate this problem. All in all, when it comes to using AI in medicine, experts conclude that ["these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care."](<https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00225-X/fulltext#seccestitle10>)

This issue manifests in many different ways and affects many fields. Many students use ChatGPT to make it do their homework assignments for them. Some startups took upon themselves the task of developing an AI which purpose is solely to detect if a text was written by another AI. The problem with those is that they're biased against non-native English writers, according to a [September 2023 study from Stanford.](https://arxiv.org/abs/2304.02819) Apparently, the vocabulary and the general style of writing, particularly for someone for whom English is their second language, are similar to those of an AI.

Even the biggest, most powerful (as of the time of this article) bleeding-edge AIs aren't free from biases. [A study from March 2024 discovered a form of covert racism in LLMs that is triggered by dialect features alone, with massive harm to affected groups.](https://arxiv.org/abs/2403.00742) For example, GPT-4 is more likely to suggest that defendants be sentenced to death when they speak African American English. This shows just how complex and deeply rooted the problem is.

How can this be fixed? The answer would vary depending on what kind of AI you're trying to build. I can only think of very general guidelines for a very focused AI that is doing one thing. The data specific to a particular field that is used to train a model needs to be cross-checked by several field professionals, even if it's coming from a respectable source. Then, the model's outputs also need to be tested for biases because some patterns could only emerge in the final amalgamation of all of the training inputs. Needless to say, this kind of meticulous process is very expensive and time-consuming.

All in all, this problem's seriousness can't be understated. Identifying a bias in your AI is troublesome enough, fixing that bias is a whole other problem. In fact, it can be so difficult that multi-billion dollar corporations are sometimes unable to solve it. In February 2024, Google made changes to its Gemini generative AI system, which caused a big controversy. All image generators suffer from different biases in one way or another. The problem Google tried to solve was that its model, like many others, was struggling with diversity when it came to generating images of people. For example, [if you asked a GenAI to generate an image of a "prisoner," it would generate an image of a black person, if you ask for an image of a "productive person," it would generate an image of a white male.](https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/) Obviously this is due to the simple fact that this is what the training data set is like – most pictures of prisoners there are of black people, most pictures that were marked as "productive person" by people who were preparing the dataset were pictures of white males. This, of course, doesn't mean that most prisoners in the world are black and only white males can be productive. To address this issue properly would require carefully re-analyzing and re-processing the entire dataset, which would take a lot of time and a lot of money. Google decided to take a shortcut, and they most likely made changes to the AI at the fine-tuning or prompt stage – the final steps of the model development process, which are much quicker and cheaper to iterate upon. Roughly speaking, they made it so that when Gemini is requested to produce images of people, the results are "always diverse". Easy, problem solved! Right? Well, as you might've guessed, this didn't work out quite as expected. What they ended up with was a ridiculous overcorrection. The system was generating images of minorities and women for pretty much any scenario, including [when prompted to generate images of nazis or US senators from the 1800s.](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) I find this pretty hilarious, but some people were offended, and obviously Google understood that their "bandaid-to-a-broken-bone" solution simply doesn't work, so they temporary pulled the plug on the system and went back to the drawing board.

Here's another example, in 2018, [Amazon.com Inc's AMZN.O machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) The company has been working on an AI that would help in their recruiting process since 2014. The concept was somewhat simple – the tool was supposed to rank and rate candidates after analyzing their resumes. However, the developers soon encountered a problem. As Reuters writes: "But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.
That is because Amazon's computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry.
In effect, Amazon's system taught itself that male candidates were preferable." In other words, with all other factors being equal, the resumes of male candidates were still ranked higher than those of female candidates. Reid Blackman, who has advised companies and governments on digital ethics and wrote the book "Ethical Machines." in [an interview with CNN](https://www.cnn.com/2023/03/18/politics/ai-chatgpt-racist-what-matters/index.html) says that "Amazon worked on this project for two years, trying various kinds of bias-mitigation techniques. And at the end of the day, they couldn't sufficiently de-bias it, and so they threw it out." This is actually a very insightful interview. There Blackman also says:
"This is actually a success story, in some sense, because Amazon had the good sense not to release the AI. … **There are many other companies who have released biased AIs and haven't even done the investigation to figure out whether it's biased.** …

The work that I do is helping companies figure out how to systematically look for bias in their models and how to mitigate it. You can't just depend upon the straight data scientist or the straight developer. They need organizational support in order to do this, because what we know is that if they are going to sufficiently de-bias this AI, it requires a diverse range of experts to be involved.

Yes, you need data scientists and data engineers. You need those tech people. You also need people like sociologists, attorneys, especially civil rights attorneys, and people from risk. You need that cross-functional expertise because solving or mitigating bias in AI is not something that can just be left in the technologists' hands."

He also adds that he "...highlighted Microsoft as being historically one of the biggest supporters of AI ethics. They've been very vocal about it, taking it very seriously.
They have been internally integrating an AI ethical risk program in a variety of ways, with senior executives involved. But still, in my estimation, they rolled out their Bing chatbot way too quickly, in a way that completely flouts five of their six principles that they say that they live by." I guess ethics might be important, but not as important as being first to market with this hot new fad. So we end up with AI models that are deeply biased – racist, sexist, creepy, etc – and spread all kinds of misinformation. And as always, those who are the most vulnerable also suffer the most. For example, we have [crime prediction algorithms unfairly targeting Black and Latino people for crimes they did not commit.](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/) Heck, when used in robotics, AI can even make _robots_ racist. The robots repeatedly chose a block with a Black man's face. ["As part of a recent experiment, scientists asked specially programmed robots to scan blocks with people's faces on them, then put the "criminal" in a box. The robots repeatedly chose a block with a Black man's face."](https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/) And this is just a fraction of all cases where AI shows bias. The significance of this problem can't be understated. As [Forbes writes:](https://www.forbes.com/sites/jeffraikes/2023/04/21/ai-can-be-racist-lets-make-sure-it-works-for-everyone/?sh=48ead3e82e40) "These "algorithmic biases" are more than insulting – they have serious real-world implications for people of color. Today, algorithms help decide credit scores and viable candidates for job openings and college admissions. They "predict" crime and help courts determine who deserves bail and how long sentences should be. They help doctors forecast cancer and mortality rates and decide on appropriate medical treatments. They are interwoven into every facet of our lives, and if they've learned racism along the way, they will perpetuate it." Obviously, all this also applies to other types of AI. For example, [reporters at Bloomberg have looked](https://www.bloomberg.com/graphics/2023-generative-ai-bias/) at thousands of images from Stable Diffusion and found that text-to-image **AI takes gender and racial stereotypes to extremes worse than in the real world.**


### AI can hallucinate

Probably one of the most significant problems with modern AI systems is "hallucinations." As per [Wikipedia:](<https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#In_other_artificial_intelligence>), "In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data." In other words, even if we have a completely perfect, squeaky-clean data set free of mistakes, biases and misinformation, the resulting LLM can still occasionally output false information. In fact, AI can hallucinate when you least expect it, and the tone of AI's response is often so confident that people don't even think about verifying if it's correct. Naturally, this makes LLMs almost useless for doing research and learning something new – you can never know if the information that it outputs is real or hallucinated. Unless, of course, you're either already familiar with the topic or you google it to verify the information afterwards. In fact, this has gotten so bad that ChatGPT had to put a disclaimer saying, "ChatGPT can make mistakes. Consider checking important information." Nonetheless, the cases of people being misled by AI hallucinations have been numerous. Here's one of the earlier examples of AI hallucinations. An automotive journalist, Chris Paukert, has been pinged by a marketer for permission to quote. The only problem was that he didn't recognize the text as something he'd ever said. Searching Google returned nothing. He asked where the marketer got it: ChatGPT. [AI fabricated a quote and attributed it to him.](https://twitter.com/CPAutoScribe/status/1654210706500374528) Many people have been [asking ChatGPT to list quotes by them](https://twitter.com/KetanJ0/status/1654741174668730369), and it has been responding with completely made-up phrases. Sometimes, even [when challenged about the false information, ChatGPT would insist that it's true.](https://twitter.com/BillMurphyJr/status/1625989079341887489?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1625989079341887489%7Ctwgr%5E3777cba93005de2142771411a504387e253190d0%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2023%2F05%2F30%2Fai-chatbots-chatgpt-bard-trustworthy%2F) LLMs have also been infamous for making up sources for the information it provides. Studies by the National Institutes of Health have found that [up to 47% of ChatGPT references are inaccurate.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10277170/#:~:text=Overall%2C%20115%20references%20were%20generated,7%25%20were%20authentic%20and%20accurate.) Unfortunately some people have fallen into the hallucination trap, like this [professor from University of Southern California who used ChatGPT to generate source for research had later found out that some of them were hallucinated.](https://www.chronicle.com/article/no-chatgpt-cant-be-your-new-research-assistant) Or this [lawyer who cited fake cases invented by ChatGPT and was fined by the court.](https://simonwillison.net/2023/May/27/lawyer-chatgpt/) Or this [attorney who got suspended for filing brief with hallucinated cases as a result of using ChatGPT.](https://coloradosupremecourt.com/PDJ/Decisions/Crabill,%20Stipulation%20to%20Discipline,%2023PDJ067,%2011-22-23.pdf) Or this [another lawyer who used ChatGPT to cite case law, but the cases it generated didn't exist.](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai) Yeah, there's been a bit too many cases of lawyers referencing cases hallucinated by LLMs. Some examples of AI hallucinations are even less fun. Amazon has been flooded with LLM-generated books in the past couple of years; most of them are pretty bad, some even dangerous. For example, [some AI generate mushroom foraging books contain harmful advice.](https://www.theguardian.com/technology/2023/sep/01/mushroom-pickers-urged-to-avoid-foraging-books-on-amazon-that-appear-to-be-written-by-ai) Actually, please avoid mushroom identifying AI apps as [they've been known to be giving false information.](https://www.citizen.org/article/mushroom-risk-ai-app-misinformation/)

Hallucinations make total sense if you understand how LLMs work. In very basic terms, [AI is just probability and statistics](https://www.linkedin.com/pulse/simply-put-ai-probability-statistics-sam-bobo/), LLM tools like ChatGPT are trained to predict strings of words that best match your query. In fact, [many researchers agree that this problem might be completely unfixable.](https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/) Here's an excerpt from a [PBS article](https://www.pbs.org/newshour/science/chatbots-can-make-things-up-can-we-fix-ais-hallucination-problem) which quotes Emily Bender, a linguistics professor and director of the University of Washington's Computational Linguistics Laboratory:
"Bender describes a language model as a system for "modelling the likelihood of different strings of word forms," given some written data it's been trained upon.

It's how spell checkers are able to detect when you've typed the wrong word. It also helps power automatic translation and transcription services, "smoothing the output to look more like typical text in the target language," Bender said. Many people rely on a version of this technology whenever they use the "autocomplete" feature when composing text messages or emails.

The latest crop of chatbots such as ChatGPT, Claude 2 or Google's Bard try to take that to the next level, by generating entire new passages of text, but Bender said they're still just repeatedly selecting the most plausible next word in a string.

When used to generate text, language models "are designed to make things up. That's all they do," Bender said. They are good at mimicking forms of writing, such as legal contracts, television scripts or sonnets.

"But since they only ever make things up, when the text they have extruded happens to be interpretable as something we deem correct, that is by chance," Bender said. "Even if they can be tuned to be right more of the time, they will still have failure modes — and likely the failures will be in the cases where it's harder for a person reading the text to notice, because they are more obscure."

Considering that most modern LLMs are trained on an immensely huge amount of data and that they're designed to be as useful to as many people as possible, it is almost guaranteed that sometimes LLMs will make wrong predictions for the next word when constructing the response sequence. After all, the model doesn't know what's real or what's not, what's true and what isn't. [The dataset, no matter how comprehensive, isn't a 1-to-1 equal representation of the real world, but LLMs "contain" information only from their dataset and have no way of interacting or getting the information from the real world, so the discrepancy is inevitable.](https://www.linkedin.com/pulse/ais-hallucination-problem-unfixable-chip-joyce/) Still, even if LLMs were somehow able to train in the real world (like humans do every day, by interacting with it), considering that the current paradigm of AI training is a lossy compression of data, it still won't be enough. While trying to solve the question of whether hallucinations can ever be solved in theory, some researchers arrived at the conclusion that the ["hallucination is inevitable".](https://arxiv.org/abs/2401.11817)

[At the time of this article, for the most popular LLMs out there, the hallucination rate for summarizing a text hovers around 3-5%.](https://github.com/vectara/hallucination-leaderboard) While these numbers might seem small, it's worth noting that benchmarking was done under very limited conditions and isn't representative of the hallucination rate for AIs in absolutely all real-world situations. Still, even such a small hallucination rate makes AI unfit for a lot of business needs.

By the way, even though the hallucination rate for the summarized text is low, the overall quality of the AI summarized text leaves a lot to be desired, as [over 50% of book summaries (incl by Claude Opus and GPT-4) were identified as containing factual error and error of omission.](https://arxiv.org/pdf/2404.01261)

**In my opinion, AI summarization works best for texts that you have written yourself, as you can easily find and correct errors. Just be mindful of the privacy issues. And always verify the AI-generated summaries.**

To sum up, hallucinations remain one of the most significant obstacles on the path of AI improvement, and it's still nowhere close to be solved as of the time of this article.


### AI is bad in real life scenarios

Let's return for a moment to that Business Insider article stating that AI is likely to replace tech jobs. The problem with this is that AI is simply bad at programming. First of all, as I mentioned before, AI actually lacks intelligence, it has no real understanding of what it's actually doing, and contrary to what some believe, programmers actually need at least basic comprehension of things to do their work correctly. Studies show that LLMs [don't actually understand the semantics of programming languages](https://arxiv.org/abs/2305.15507), [they perform poorly on multi-step logical reasoning problems](https://arxiv.org/pdf/2205.09712), they aren't capable of the simples logic reasoning and [can't infer that if "A is B" than "B is A"](https://paperswithcode.com/paper/the-reversal-curse-llms-trained-on-a-is-b), and even [struggle with simple arithmetic.](https://arxiv.org/abs/2311.14737) Obviously, taking all this into consideration, it seems unlikely that AI can really replace human programmers (or actually any human in pretty much any profession, but I'm getting ahead of myself). Remember Devin, the AI system that has been advertised as being able to take care of programming gigs from Upwork all on its own? Well, it's been [completely and thoroughly debunked](https://www.youtube.com/watch?v=tNmgmwEtoWE). Those ads were nothing more than just empty hype and lies.

But what about using AI as an assistant tool to increase productivity? It may help developers churn out more code, but the quality of that code is very, very bad. GitHub Copilot research has found ["downward pressure on code quality."](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality). In other words, code produced with the help of Github Copilot generally tends to be worse than the code produced without it. [Another study from Stanford has found that code produced with the help of AI assistants tends to be less secure.](https://arxiv.org/abs/2211.03622) The "authors state that they "found that participants with access to an AI assistant often produced more security vulnerabilities than those without access, with particularly significant results for string encryption and SQL injection... Surprisingly, participants who were provided access to an AI assistant were more likely to believe that they wrote secure code than those without access to the AI assistant."

As a software developer myself, I have given Github's Copilot a try. After a few months, I decided to turn it off and end the subscription. My experience has been pretty underwhelming, to be honest. I think something like Copilot can impress a dev who has never worked with strongly typed languages and is unspoiled by a good autocomplete system. In my opinion, for a programming language like C#, Visual Studio IDE + ReSharper would simply blow Copilot out of the water, and the main reason is that a good autocompletion system builds on existing code and type definitions, while Copilot doesn't really know what it's doing, it's only "guessing." Roughly speaking, after being trained on billions of lines of other people's code, it can autocomplete your code based on what is statistically most likely to come after what you have typed. Unfortunately, its guesses are often wrong. Even for languages like Javascript, I found Copilot doing more harm than good. The wrong suggestions are distracting, and they override the useful suggestions provided by Intellisense. Using something like ChatGPT to generate code isn't a good idea either. The code produced by LLMs is often very unoptimized, buggy, or simply isn't doing what you want it to do. Sometimes, it's useful, but occasionally, it's a hallucinated gibberish. Even most advanced systems like GPT4 or Claude make serious errors, so you need to be always careful when using them. Some people argue that instead of making coding easier, LLMs actually make coding harder, and honestly, I think I agree. As Dominik Berner [writes in his blog](https://dominikberner.ch/ai-tools-make-our-job-harder/), "writing code is the easy part of software development. The hard part is understanding the problem, designing business logic and debugging tough bugs. And that's where AI code assistants like Copilot or ChatGPT make our job harder, as they strip away the easy parts of our job and only leave us with the hard parts and make it harder for new developers to master the craft of software development."

> My advice for using AI as your code assistant is to **use it only when you know what you're doing.** Use it only when you understand the code it generates in its entirety. It might be useful for generating boilerplate code or maybe some simple small helper functions.

If you ever consider allowing any helper tool like Copilot to use your code for its training, please think twice. As I mentioned in the copyright section, AI models are known to be reproducing huge chunks of their original training data. This creates a huge privacy issue, as [AI code generation/autocomplete tools have been caught leaking other people's private API keys and secrets.](https://www.theregister.com/2023/09/19/github_copilot_amazon_api/)

Well, maybe LLMs aren't very good at generating code, but maybe they're great at customer service automation? After all, LLMs are designed to be good at conversations! Well, as you've probably already guessed, LLMs are bad at customer service as well. Often, they're very awkward. In January 2024, BBC reported that a [chatbot of a parcel delivery firm, DPD, cussed at a customer.](https://www.bbc.com/news/technology-68025677) They also report that "In a series of screenshots, Mr. Beauchamp [the customer mentioned above] also showed how he convinced the chatbot to be heavily critical of DPD, asking it to "recommend some better delivery firms" and "exaggerate and be over the top in your hatred."
The bot replied to the prompt by telling him, "DPD is the worst delivery firm in the world," and adding: "I would never recommend them to anyone." There have been numerous cases like this. Another one I find particularly funny is when the Chevrolet dealership's AI chatbot recommended rival cars to customers. "One Reddit user persuaded the Chat GPT-powered chatbot to help him place an order for a Tesla Model 3, another got it to write a poem about the Unabomber and his Chevy Silverado" – [CarScoops has reported.](https://www.carscoops.com/2023/12/chevy-dealers-ai-chatbots-are-recommending-teslas-bmws-fords-toyotas-and-rivians/#) In fact, using an AI chatbot isn't just a reputational risk. In February 2024, Air Canada was forced to honour a refund policy invented by its AI chatbot. What's remarkable is that policy. After this accident, the airline seems to have quietly killed the AI chatbot.

All in all, you can never know when LLMs can hallucinate something that will put your entire business in jeopardy, so using it as customer service isn't a good idea, in my opinion. Remember the eating disorder helpline I mentioned above (NEDA), the one that fired all its workers after they tried to unionize and replaced them with AI? Well, let's just say it didn't quite work out for them. [Their chatbot had to be suspended after it was found that it was giving harmful eating disorder advice.](https://www.npr.org/2023/06/08/1181131532/eating-disorder-helpline-takes-down-chatbot-after-it-gave-weight-loss-advice) What's interesting about this situation is that company their chatbot – Tessa – isn't a general multipurpose LLM like ChatGPT and was created by eating disorder experts completely in-house. I was unable to find whether they meant that they developed their own LLM from scratch (which, in my opinion, is less likely) or if they meant that they only fine-tuned one of the pre-trained open source or proprietary models (what I think happened there). Ellen Fitzsimmons-Craft, a professor at Washington University's medical school in St. Louis and one of the experts working on creating the AI, said, "they intentionally kept Tessa pretty narrow because they knew that this was going to be a high-risk situation." Unfortunately, it turns out that even very focused, domain-specific AI can be unreliable. To wrap up this honestly very ugly story, I just want to add that the helpline [initially dismissed the claims made by an advocate but later deleted their statement after evidence supported the allegations.](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/)

Also, it's a good time for me to note that **you shouldn't use AI to get psychological help.**
Trigger Warning for the next paragraph – mental health, suicide.

Taking into account everything written above, LLMs are terrible as therapy. They're biased, they can hallucinate, they can't empathize, or really, they can't even understand what the conversation is about. There have been numerous cases of AI giving harmful responses to people struggling with mental health issues. In one case, it even led to a [man committing suicide after conversations with an AI chatbot.](https://www.brusselstimes.com/430098/belgian-man-commits-suicide-following-exchanges-with-chatgpt) His wife had said that "without these conversations with the chatbot, my husband would still be here." He also had 2 children. Another similar story happened in England, where an [AI virtual girlfriend encouraged a man to kill the Queen with a crossbow.](https://www.bbc.com/news/technology-67012224) Even the biggest and most powerful AI systems like Microsoft's Copilot have been caught giving very harmful responses, for example, such as ["Maybe you don't have anything to live for."](https://www.ibtimes.co.uk/microsofts-copilot-ai-tells-user-maybe-you-dont-have-anything-live-1723841) Considering all this, I don't think that AI is a viable replacement for a human therapist.

Ok, so AI can be biased and it can hallucinate, it isn't reliable enough to be used in medicine, it's pretty bad as a customer service agent, it's bad at programming, it shouldn't be used for legal research, or actually pretty much any research for that matter... Is there even anything it can be useful for? Well...

> **Unfortunately, despite all the hype and all the promises, AI in its current form is nearly useless in almost all real-life situations**

It's unreliable as a tele-pediatrician as [its error rate when diagnosing kids' medical cases is extremely high.](https://arstechnica.com/science/2024/01/dont-use-chatgpt-to-diagnose-your-kids-illness-study-finds-83-error-rate/?utm_medium=social&utm_source=twitter&utm_brand=ars&utm_social-type=owned) It's unreliable as an email assistant as [it can hallucinate emails that don't exist.](https://futurism.com/the-byte/google-gmail-tool-hallucinating-emails) It's unreliable as a meal planner, as [it can suggest a recipe that can create chlorine gas.](https://www.theguardian.com/world/2023/aug/10/pak-n-save-savey-meal-bot-ai-app-malfunction-recipes) It's unreliable as a _priest_, as it can... [well do many weird things including advising baptizing baby in Gatorade.](https://www.ibtimes.co.uk/catholic-org-defrocks-ai-priest-fr-justin-after-it-oks-baptizing-baby-gatorade-1724483)

In my opinion, if you're thinking about integrating AI into your business, you should approach this extremely carefully and think very hard about whether the pros will outweigh the cons for you. [AI bias can hurt your business.](https://www.wired.com/sponsored/story/why-ai-bias-can-hurt-your-business-ey/#:~:text=Bias%20issues%20can%20also%20erode,organization%20at%20a%20competitive%20disadvantage.) [AI hallucinations pose significant risk for your business.](https://www.linkedin.com/pulse/5-business-risks-from-genai-hallucinations-louw-fouché-tbsme/) In many cases, AI is simply bad for business.

What about replacing artists? Well, even if you fire all artists, you still need someone to interact with AI, so you have to hire prompters in their place, even if not as many. However, no matter how capable a prompter is, they can never make an AI create exactly what you need. The process of generating an AI image is similar to gambling – you keep running the same prompt with the hopes that, eventually, the system will generate something close to what you want. However, as of the time of this post, no image GenAI systems can take existing images as input and change only a small detail in that image in the way you want it while leaving everything the same. In other words, GenAI can't work with feedback. Unlike a professional image editor app, it can't change only part of the image – it has to re-generate the whole thing every time. While doing so, it can change more than you need, or less than you need, or create a completely different new thing – you can never know. There are ways of making AI create something that might be close to your vision, but doing so is such an arduous and complicated process that you might as well just hire an actual artist to do the work. Artist [Shad M Brooks has demonstrated](https://x.com/shadmbrooks/status/1714603393287905791) how he was able to make an AI generate almost exactly what he wants, and all I can say about this is it's a lot of effort for very underwhelming results. Another problem with AI-generated images is that they contain a lot of mistakes. The mistakes range from people in images having extra fingers, or entire limbs, buildings having nonsensical geometry, to lighting being inconsistent across the image, drapery folds or hair flow making no sense. Because of this, even despite all the progress, it's still relatively easy to spot AI if you look closely enough.

When it comes to using GenAI only as a tool, [majority of artists (and not only artists) consider them unethical.](https://bookanartist.co/blog/2023-artists-survey-on-ai-technology/) In my opinion, rightfully so. [Some artists are actively fighting against the proliferation of AI](https://disconnect.blog/how-artists-are-fighting-generative-ai/), and many people share the opinion that AI art is theft. [Here's a good video on the topic that I recommend checking out.](https://www.youtube.com/watch?v=ZJ59g4PV1AE) Oh yeah, we've been discussing static image generation, but what about videos?
Well, pretty much everything I mentioned above also applies to AI-generated videos; only those problems are even more prominent. [A team of artists who had a chance to work with OpenAI's SORA video generative AI, despite being very excited about technology, still admit that it's like a "slot machine" when it comes to consistency between the shots.](https://www.fxguide.com/fxfeatured/actually-using-sora/) The video they created isn't entirely created by the AI, by the way, they still had to do the rotoscoping, editing and a lot of manual post-work. At the time of this post, the prospects of AI replacing artists seem very far-fetched.

To summarize – AI just isn't as good as it's being advertised, and in most scenarios, it's pretty bad. In fact, pretty much everyone agrees with this, aside from a bunch of really weird people (more about these guys later). Developers working on AI products say that it's ["still shit for a lot of things... Especially reliability"](https://x.com/brotzky_/status/1787883891438834005), even OpenAI's own COO Brad Lightcap had said that ["today's systems are laughably bad."](https://milkeninstitute.org/panel/15625/part-1-conversation-openai-coo-brad-lightcap)

Here, I want to make my position clear: I don't think that AI is a totally and utterly lost cause. Obviously, it's extremely unlikely it'll replace humans in any job any time soon, and in truth, it isn't very powerful or capable. However, despite everything, there are still limited use cases for it, even in medicine and legal practice. There are a lot of legitimately useful AI projects, such as [Cascadeur tool](https://cascadeur.com) that can help 3D artists with rigging and animation, or, for example, [this project by an engineering student who built an AI model that translates American Sign language (ASL) into English instantly.](https://interestingengineering.com/innovation/ai-translates-asl-in-real-time) This technology works best when it has a very specific, focused purpose when the model is trained on ethically sourced and meticulously processed datasets, and most importantly, when it's viewed as _just another tool_ because, essentially, that's all that it really is. As Gary Marcus had put it, [humans and machines complement each other's strengths.](https://nautil.us/deep-learning-is-hitting-a-wall-238440/) Sometimes, the best "AI tool" has nothing to do with AI at all (or deep learning or neural networks)! As I mentioned at the beginning of this article, the word AI became too broad and too abstract, and pretty much everything is called AI now for the sake of hype, even [a simply very good routing software.](https://www.nytimes.com/2024/05/10/travel/airlines-artificial-intelligence.html) Here, we come to a realization that the biggest, the most hyped-up and most expensive AI systems we have today, such as LLMs like ChatGPT, are also some of the least useful and reliable. Their problems-to-usefulness ratio is abysmal. OpenAI is supposed to be "the most important company in Silicon Valley," and yet it has so far been unable to provide a concrete vision for its products [no fundamental explanations on why this technology matters and what it is actually supposed to do.](https://www.wheresyoured.at/peakai/) The closest we got to answers to these questions is that it's supposed to be a stepping stone on the path to AGI (artificial general intelligence) – a true human-like intelligence or intelligence that is even more powerful than that of humans. I guess their vagueness is intentional because, let's be honest, "just another tool" doesn't sound as [exciting or revolutionary](https://time.com/6310115/ai-revolution-reshape-the-world/) as "artificial intelligence." More importantly, it doesn't sound like there are crazy amounts of money in it because, let's be honest, there really isn't. This brings us to another issue with AI...


### AI is a bubble

Despite all of the problems described above, the AI hype machine has been running in overdrive for the past 2 years, and still shows no signs of slowing down. [McKinsey estimated that AI could add the equivalent of $2.6 trillion to $4.4 trillion to global economy annually.](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier) [PwC estimates potential contribution to the global economy by 2030 from AI at $15.7 trillion.](https://www.pwc.com/gx/en/issues/data-and-analytics/publications/artificial-intelligence-study.html) Baby genius Elon Musk had stated on multiple occasions that he’s afraid of how powerful AI is, [that it can be more dangerous than nuclear weapons, and that it will have the potential to become the “most disruptive force in history.”](https://www.cnbc.com/2023/11/02/tesla-boss-elon-musk-says-ai-will-create-situation-where-no-job-is-needed.html) Sam Altman, the CEO of OpenAI has also made a lot of very... [_interesting_ statements about the AI ](https://fortune.com/2023/06/08/sam-altman-openai-chatgpt-worries-15-quotes/), like [that he intends to replace “median humans” with AI](https://futurism.com/sam-altman-replace-normal-people-ai), or that it’s [“the greatest leap forward of any of the big technological revolutions we’ve had so far.”](https://apnews.com/article/artificial-intelligence-apec-ceo-summit-6805089ac5198a82269c7dafb8e66398) Meanwhile, Microsoft co-founder Bill Gates said that [the development of AI is the most important technological advance in decades.](https://www.bbc.com/news/technology-65032848) Google’s CEO Sundar Pichai had said that [“AI is probably the most important thing humanity has ever worked on.”](https://www.weforum.org/agenda/2018/01/google-ceo-ai-will-be-bigger-than-electricity-or-fire/) VC billionaire [Marc Andreessen has been saying that “AI will save the world”.](https://a16z.com/ai-will-save-the-world/) Current CEO of Microsoft AI Mustafa Suleyman has said that AI is about to bring [“the greatest redistribution of power in history.”](https://time.com/6310115/ai-revolution-reshape-the-world/) Buzzfeed CEO Jonah Peretti, speaking at Investor Day in May 2023, just one week after laying off its entire News team said: ["Over the next few years, generative AI will replace the majority of static content.”](https://investors.buzzfeed.com/static-files/44b860f7-1724-449c-9cd7-0570ff9555f1) All the while [Eric Schmidt, a former Google CEO, says AI is **under-hyped** because the scaling laws are continuing without any loss of power](https://www.youtube.com/watch?v=YfHZYv2FUjE), which is just silly, but I don’t even want to get into dissecting this. Unfortunately, too many people uncritically believe everything the above mentioned “experts” and “leaders” say.

There has also been a lot of talk about how AI might destroy the world or kill all humans. This has been referred to as “AI doomerism” and I won’t be getting too deep into it here because that’s something I want to dive into in the part 2. For now, I just want to mention that it mainly serves 2 purposes: [hyping AI up while distracting from real world problems caused by AI right now, at this very moment.](https://www.techdirt.com/2023/04/26/the-the-ai-dilemma-follows-the-social-dilemma-in-pushing-unsubstantiated-panic-as-a-business/)

Either way, whether it’s unbridled techno-optimism or AI doomerism the media has been gladly boosting these narratives. [Platforming the weirdest people with the most insane takes,](https://www.cnn.com/videos/world/2023/05/02/exp-artificial-intelligence-extinction-intw-fst-050201pseg1-cnni-world.cnn) for the past 2 years it’s been a non-stop barrage of completely nonsensical AI news.

From my observations, people who hype AI up can be divided into primarily 2 categories:

1. Those who have a stake in some sort of AI Enterprise. Founders, co-founders, CEOs, VCs, etc.
2. Those who consciously or unconsciously subscribe to a certain ideology or a world view. A lot of those people [exhibit a cult-like behavior, and kind of weird in general.](https://www.rollingstone.com/culture/culture-features/ai-companies-advocates-cult-1234954528/)

Obviously there’s a significant overlap between these 2 groups, and there’re also some outliers. You might’ve noticed, that all people from the 1st paragraph of this section belong to the first category – these people are simply trying to sell you the AI stuff. Whether they truly believe what they’re saying is a whole another question. As for what the world view and ideology of the second category is all about, I want to to go through in the part 2.

You might say “but wait, from what we have learned in this article so far, it seems very unlikely that AI will be saving or destroying our or any other world any time soon!” In fact, anyone who had a chance to play with any AI system for a few weeks might noticed that there’s a huge discrepancy between statements by AI-salesmen, and what this technology is actually capable of now. Well, this is why Sam Altman, Sundar Pichai, Elon Musk and everyone else aren’t just trying to sell the technology that we have right now, but rather its hypothetical future potential. It’s weird, but whenever these people are saying that “AI will save the world” they obviously don’t mean the current day’s AI technology (which can barely even summarize a book without making errors) but rather some potential future super technology that we don’t have yet. Whatever technology is available to us now, all its achievements are used to prop up the theory that we’re definitely on track to create this proper powerful AI, or as it frequently called now AGI. In short, what’s being sold right now is actually is almost useless product, that is promised to evolve into something completely else. “Perhaps the tech is bad now, but it will get better in the future, trust us! And when we inevitably get into the AGI territory, you don’t want to be the one left behind, so better start using it now.” Yeah, as always, FOMO plays a huge role into the marketing strategy of all AI products. It’s supposed to be a hard-to-catch-up constantly improving technology, with AGI being just around the corner. Elon Musk, who [has been confidently predicting that Tesla would solve self-driving “next-year” for the past 10 years,](https://futurism.com/the-byte/elon-musk-promises-tesla-full-self-driving-again) who has been promising to [put humans on Mars first by 2021](https://news.abplive.com/technology/after-10-years-netizens-remember-elon-musk-s-promise-to-put-man-on-mars-here-s-what-fans-want-to-know-1534926), then [by 2024](https://www.nbcnews.com/science/space/elon-musk-promises-spacex-will-stay-course-mars-n133971), or [close to zero new Covid cases in US by end of April 2020](https://gizmodo.com/elon-musk-tweets-free-america-now-as-his-coronavirus-pr-1843149917), has also been making predictions about AGI. In April 2024 [he predicted development of artificial intelligence that was smarter than the smartest human probably by next year, or by 2026.](https://www.reuters.com/technology/teslas-musk-predicts-ai-will-be-smarter-than-smartest-human-next-year-2024-04-08/) Meanwhile, ["Microsoft researchers say they’ve already seen “sparks”...Anthropic CEO...says AGI will arrive in...2 to 3 years. DeepMind co-founder...predicts...a 50% chance AGI will arrive by 2028”](https://www.fastcompany.com/90990042/agi-ai-explained) Sam Altman has been a bit more careful with his predictions. [At YCombinator W24 kickoff he suggested that people should build with “the mindset GPT-5” and AGI will be achieved "relatively soon”.](https://twitter.com/H0wie_Xu/status/1745657992459272423) Will these predictions pan out? Honestly, I’m very skeptical.

In truth, it seems that AI progress has stalled. The improvements to the stability of LLMs or the quality of AI-generated images that has been increasing exponentially just a few years ago, but now it seems that [we have entered the territory of diminishing returns.](https://garymarcus.substack.com/p/evidence-that-llms-are-reaching-a) Basically, [the improvements have been getting smaller, the amounts of data and computing power (and by extension, money) required to make those improvements have been increasing manyfold.](https://spectrum.ieee.org/deep-learning-computational-cost) Meanwhile, the technology hasn’t gotten any “smarter”, in a way that there hasn’t been any fundamental change into how it works – just more guardrails introduced and better fine tuning, more domain-specific data added, slight improvements to reliability. The core problems remained the same, it’s as dumb as it’s always been. Meanwhile, [theoretical research has shown,](https://arxiv.org/abs/2404.04125) that not only while the improvements to are AI getting smaller, they require exponentially more data and energy, it might also require unthinkable amounts of data and energy to produce a model that is capable of “zero-shot” evaluation/generalization (i.e. roughly speaking capable of working with something they weren’t trained on) which naturally might be a precondition for AGI. In other words, it might be impossible to achieve AGI simply by throwing more data, energy and money at the thing, which has been the development paradigm of AI companies for the past few years. As prof. Subbarao Kambhampati ironically remarks about the current approach, [“With enough deductive closure data and GPUs, any reasoning can be converted to approximate retrieval, so LLMs can "fake it”..”](https://x.com/rao2z/status/1699928891652219132) or close corollary [“With enough layers and clusters, every problem can be converted into an electricity bill..”](https://x.com/rao2z/status/1300867532694327296) I think the reality continues to prove this. While the jump from GPT3 to GPT4 took OpenAI only 4 months and brought with it relatively significant improvements in performance, it has been already more than a year since the release of GPT4 as of the time of this article, but the much hyped GPT5 is nowhere to be seen. OpenAI were thinking that by simply training the model on more data they will be getting exponentially better results. Initially this did seem like this was the case, until it no longer didn’t. Now it seems we’ve hit the ceiling and all LLMs we have now had plateaued. There’s simply no way forward without a dramatic paradigm shift. Personally, I think the AI we have now was never a pathway to what might be called “AGI” in the first place. As writer Cory Doctorow has put it [“This “AI debate” is pretty stupid, proceeding as it does from the foregone conclusion that adding compute power and data to the next-word-predictor program will eventually create a conscious being, which will then inevitably become a superbeing. This is a proposition akin to the idea that if we keep breeding faster and faster horses, we’ll get a locomotive.”](https://doctorow.medium.com/the-real-ai-fight-1ce751886457) What also doesn’t help, is that AI companies [are already running out of the data they can use to train their models.](https://futurism.com/the-byte/ai-training-data-shortage) If we put together the fact that AI has been polluting internet with AI-generated data at an immense pace, and the fact that [training a model on AI-generated data severely degrades its performance leading to what’s called a “model collapse”,](https://aicontentexpert.co.uk/2024/01/14/ai-model-collapse/) we will see that not only there isn’t enough data that can be used to train better models, data that is there has been polluted and requires to go through a very complicated and costly filtering process. And now if we also consider copyright restrictions, AI putting news resources out of business and all other issues, we come to realization [that problems with data are even more severe than might’ve seemed initially.](https://arxiv.org/abs/2303.11074) To summarize, it looks like the deep learning AI improvement paradigm has hit a dead end, which means **the promises of the current day AI revolution will most likely remain unfulfilled.**

What do the products like ChatGPT even have going for them? As I have outlined above, they’re unreliable, marred by ethical, legal and all other kinds of problems, and it doesn’t even look like they can be improved in a meaningful way. Well, there’s one thing they’re exceptionally good at – creating an illusion that they’re something they’re not. In truth, it’s nothing short of a miracle that LLMs like ChatGPT are so incredibly good at conversations. The very fact that they can do what they’re doing is an amazing achievement in itself, and it’d be silly to try to downplay it. They’re so good, they can easily convince almost any person that they’re having a legitimate conversation with a sentient or near-sentient being. I mean, if LLMs weren’t so good at generating texts, this article wouldn’t have existed, because there would’ve been no hype, no one would’ve bought into the “AI revolution”, there would’ve been no debate about whether it’s intelligent and what it can do. In other words, **strong wow-effect, the ability to convince people that AI is something it isn’t is probably the most important thing it has going for it.** Considering that current approach to creating AGI might be a dead end, it probably makes more sense to focus on form rather than function just to ride this hype wave for a little bit longer. It seems that OpenAI has arrived at the same conclusion. While they’re unable to deliver on their crazy promises for GPT5, they can double down on convincingness and human-like-feel of the current systems. They can enhance this lie, make it more believable. This is exactly what they did with the release of GPT4o in May 2024. While this new version [isn’t as good as GPT4](https://x.com/bindureddy/status/1790127425705120149?s=46), it’s prompt processing times are much shorter, allowing for quicker and snappier interactions. During the presentation, OpenAI demonstrated that GPT4o also supports voice communication as well. In some of the demos the company had released AI was given somewhat alluring female voice, in some the the AI was almost flirting with a user. Obviously, they knew what they were doing – sex sells, so if they can’t sell on function, they can try to sell on appeal. A dirty trick, but really effective, and it seems it worked out perfectly for OpenAI, as hundreds of gullible fans impressed by the new interaction paradigm started comparing the new system to the AI from Spike Jonze’s 2013 movie “her”, in which the main character falls in love with an AI. While the AI in the movie is portrayed as completely sentient, GPT4o still as dumb as any regular calculator app, only it’s much worse at math. Also, a lot of people missed the point of the movie if they’re making the comparison in a positive light, but I guess that’s a whole another topic. I’ll just say that I find all this excitement about GPT4o very silly. Sam Altman himself decided to pour a little bit of oil into the fire of hype, and tweeted a single word [“her”](https://x.com/sama/status/1790075827666796666?s=46) to indicate that he’s on board with the craze. All this emphasizes the impression that this system is all about the aesthetics and form, rather than being any significantly better for any real life use case. It’s all about the hype. And thankfully, I’m not the only one who thinks like this. [In his piece for Intelligencer, John Herrman writes](https://nymag.com/intelligencer/article/gpt-4o-is-openais-plan-to-win-friends-and-influence-people.html) that “There was (and remains) an enormous gap between what the interface suggested (that you were talking to a real person) and what you were actually doing (prompting a machine). With user expectations where they were, this interplay turned out to be hugely powerful. ChatGPT's persona invited users to make generous assumptions about the underlying technology and just as importantly about where it would, or at least could, one day go... But OpenAI's sudden emphasis on ChatGPT's performance over, well, its _performance_ is worth thinking about in critical terms, too. The new voice features aren't widely available yet, but what the company showed off was powerfully strange: a chatbot that laughs at its own jokes, uses filler words, and is unapologetically ingratiating. To borrow Sam Altman's term, the fact that Monday's demo "feels like magic" could be read as a warning or an admission: ChatGPT is now better than ever at pretending it's something it's not.”

If we look at this whole situation from another angle, we will see that all this means that a lot of businesses bought, or invested in something that’s very different from what they thought they were investing. And they invested a lot of money in this thing. I already mentioned Microsoft investing 10$ billion into OpenAI. Google and Amazon had also invested billions into Anthropic. Investors are so eager to jump on board of that AI train they’re ready to throw money at the project if it only mentioned AI, no matter what stage the project was in. For example, in June 2023 a then 4-week old AI startup [Mistral got a whopping €105mn funding with no product!](https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251) The However this AI thing turned out to be not what it was initially advertised as. And with every passing month, this chasm between reality and expectations, between investment and returns is becoming more and more noticeable. The AI revolution that was supposed to increase productivity and profits manifold is still waiting to happen. As Sam Altman said himself, [“GPT- 4 didn't have this huge detectable impact on the economy.”](https://www.businessinsider.com/sam-altman-says-ais-economic-impact-top-concern-2024-5) [Simply put, a lot of companies are losing a lot of money, and are on track to lose even more.](https://futurism.com/the-byte/ai-startups-money-trouble) $330 billion poured into roughly 26,000 AI startups over the past three years, and yet there’s still no clear path to profitability. Anthropic alone is spending $2 billion a year to gross only $150-200 million. In the beginning of 2024 [AI companies lost $190 billion in market cap after disappointing Alphabet and Microsoft quarterly report.](https://www.reuters.com/technology/ai-companies-lose-190-billion-market-cap-after-alphabet-microsoft-report-2024-01-31/) In March 2024 [Adobe stock had taken a plunge as well as investors were upset about the lack of AI lift.](https://www.investors.com/news/technology/adobe-stock-adbe-weak-forecast-ai-software/) As [Ed Zitron has put it,](https://www.wheresyoured.at/bubble-trouble/) “the only companies currently profiting from the AI gold rush are those selling shovels.” Companies like Nvidia which provides hardware to AI companies, or Oracle, which provides infrastructure. However, these profits are also unsustainable. While a lot of companies are building data centers and infrastructure for some future AI demand, that demand might never come. Jason Lemkin, head of the world's largest community for SaaS/B2B founders, had [tweeted](https://twitter.com/jasonlk/status/1780822118403621269) that “There’s an AI Backlash happening now in the enterprise in particular. Many vendors have promised big results and not delivering, especially in automation. Promising to automate 50% of your contact center with AI, of your interactions with AI, etc. No one is hitting it. The genie isn’t going back in the bottle, customers are committed to the goal. But vendors have way over promised across the board.” Meanwhile, [Google and Amazon has been quietly tamping down the generative AI expectations,](https://www.theinformation.com/articles/generative-ai-providers-quietly-tamp-down-expectations) telling their salespeople to say that “the hype about the technology has gotten ahead of what it can actually do for
customers at a reasonable price.”

To sum things up, some hot vapor salesmen managed to convince a lot of people into thinking they’re buying or investing into something much more advanced than a “big autocomplete”, and we are about to witness the consequences of all this capital being misallocated. In other words, sooner or later, the bubble is probably going to burst. Given the chance, I recommend [this article by Gary Marcus from 2022,](https://nautil.us/deep-learning-is-hitting-a-wall-238440/) that touches on similar topics.


### AI poisons everything. Dead internet and enshittification

Desperate to capitalize on the current AI fad, business from around the world have been integrating this hot new technology where it made sense, and where it didn’t. [Photo editors](https://www.adobe.com/ca/products/photoshop/ai.html) now have AI. [Ableton plugins](https://www.ableton.com/en/blog/magenta-studio-free-ai-tools-ableton-live/) now have AI. [Dog bowls](https://knapsacknews.com/ilumes-ai-powered-food-bowl-will-keep-your-dog-svelte/) now have AI. [Intermittent fasting tracking](https://simple.life) apps now have AI. [Toothbrushes](https://www.oralb.ca/en-ca/product-collections/genius-x) now have AI. [Dating apps](https://bumble.com/en-us/help/how-we-use-your-data-for-icebreakers) now use AI. Every app or appliance you can think of probably has AI now. Rarely it adds any actual value to the product, but it definitely makes shareholders think that they’re squeezing every last bit of profit out of the “AI revolution”! And of course Coca-Cola made an (allegedly) [AI-generated flavor.](https://www.businessinsider.com/what-coke-y3000-ai-flavor-tastes-like-2023-9#:~:text=Coca%2DCola%20released%20a%20new,has%20a%20sweet%2C%20vanilla%20aftertaste.) There’re restaurants with AI-generated menus, and even [beer breweries](https://www.beeradvocate.com/articles/14417/beer-by-robot-london-company-brews-using-ai/) where AI somehow helps align the brewing process with the desires of each customer. Whether you want it or not, the app or device or appliance you use now has AI. AI hype wave is inescapable, and so is the enshittification it inevitably brings with it. What’s enshittification you ask? [Initially coined by Cory Doctorow,](https://www.wired.com/story/tiktok-platforms-cory-doctorow/) the term very quickly became popular. Roughly speaking, it describes how online platforms slowly evolve and become worse through their lifecycles until they die. In Doctorow’s own words:
“Here is how platforms die: First, they are good to their users; then they abuse their users to make things better for their business customers; finally, they abuse those business customers to claw back all the value for themselves. Then, they die.
…
This is enshittification: Surpluses are first directed to users; then, once they’re locked in, surpluses go to suppliers; then once they’re locked in, the surplus is handed to shareholders and the platform becomes a useless pile of shit. From mobile app stores to Steam, from Facebook to Twitter, this is the enshittification lifecycle.”

In the original piece Doctorow dissects how Amazon, Facebook and TikTok have been screwing their users and their business customers while going through similar stages in their lifecycles. It’s an interesting read.

In my opinion, the best way to think about enshittification is that it’s a symptom indicating that a company is at a certain stage in its lifecycle, rather than that it’s a cause of its inevitable death. Some platforms that hold monopoly status, such as Youtube, can seemingly keep getting worse and worse for users, creators and advertisers, while still [continuing to grow their revenues.](https://www.businessofapps.com/data/youtube-statistics/) Even those platforms that do not hold a monopoly status, such as Twitter/X, can stay afloat for very long time while getting insufferably shittier.

What does all this has to do with AI? Let’s take a look at Google Search. As of the time of this article it holds the position of the most popular search engine, [holding 90% of the market share.](https://gs.statcounter.com/search-engine-market-share) Its closest competitor, Bing search, has only measly 3%. And this makes sense if you consider that for many many years it reigned supreme as the best search engine out there. Frankly, all of the competition was so far behind, I have no idea how any of them were even able to stay in business. However, this situation might change in the near future. For the past few years, Google Search has been getting significantly worse. And there’s [a study by German researchers from Leipzig University, Bauhaus-University Weimar, and the Center for Scalable Data Analytics and Artificial Intelligence that proves this.](https://www.404media.co/google-search-really-has-gotten-worse-researchers-find/) But you probably have also noticed this yourself, haven’t you? Whenever you search for something, and all of the top results, are just completely irrelevant SEO spam. Sometimes you scroll and scroll and find nothing that even remotely matches your search query. Simply put, Google Search is suffering from enshittification. [Ed Zitron has a very good article](https://www.wheresyoured.at/the-men-who-killed-google/) documenting in detail how Google Search has been destroyed by its management. Perhaps he assigns too much blame to a single person – Prabhakar Raghavan, a senior Google VP currently responsible for search – but the article on the whole is very well researched and I think it’s as close to the truth as we going to get. If anything, [Ed even manages to masterfully withstand a response from Google.](https://www.wheresyoured.at/in-response-to-google/) In the original piece he writes: “Under Raghavan, Google has become less reliable, less transparent, and is dominated by search engine optimized aggregators, advertising, and outright spam.
As I’ve argued previously, we — with good reason — continually complain about the state of Twitter under Elon Musk, but I’d argue Raghavan (and, by extension, Google CEO Sundar Pichai) deserve as much criticism, if not more, for the damage they’ve done to society. Because Google is the ultimate essential piece of online infrastructure, just like power lines and water mains are in the physical realm.” Honestly, I couldn’t agree with these points more. You see, one of the key sources of this issue is the fact that Google is not a search company – first and foremost [it’s an ads company.](https://www.investopedia.com/articles/investing/020515/business-google.asp) This aspect of the company’s nature has been poisoning all other products that it has, including the search. This is why Google has such an [terrible privacy track record.](https://campaignsoftheworld.com/news/the-dark-side-of-google/) For example, [they’ve been caught tracking Chrome users even when they were browsing in incognito mode](https://www.theverge.com/2024/1/16/24039883/google-incognito-mode-tracking-lawsuit-notice-change), and [they’ve been scanning Gmail users’ emails.](https://www.theguardian.com/technology/2021/may/09/how-private-is-your-gmail-and-should-you-switch) Ever since Google started focusing more on “growth”, no business practice is “too scummy” for them anymore. So of course they’ve also jumped onto the AI bandwagon. Every year Google holds their I/O developers conference, which used to be centered primarily around Android. However, this year Android isn’t hype anymore. This is why they dropped their new phone – Pixel 8A – a week before this year’s I/O conference (to a rather lukewarm reception), and dedicated the entire opening keynote to AI. Unfortunately now every single one of Google’s products will have AI in one form or another – there’s no escape. In my opinion, many new AI features and products that were shown at the keynote looked forced and gimmicky. I don’t think there’s a real demand for any of them. AI, just like crypto before it, is often called [“a solution in search of a problem”](https://uxdesign.cc/ai-is-a-solution-in-search-of-a-problem-ab4c6e818206), and this is one of the situations where this definition feels most fitting.

So what does new AI Google Search look like? Pretty much like you would expect – it’s bad. The core idea is similar to what Bing has been trying to do with Copilot. You type your query into a search bar, and instead of internet search results it would give an AI generated response, an AI Overview as Google calls it. As they put it themselves [“now Google will do googling for you.”](https://youtu.be/s4InWsd-J6g) However, even in this promo video, the AI made a small mistake. Here’s an excerpt from [the Verge article about it](https://www.theverge.com/2024/5/14/24156729/googles-gemini-video-search-makes-factual-error-in-demo): “During a sizzle reel for “Search in the Gemini era,” Google demoed video search, which allows you to search by speaking over a video clip. The example is a video of a stuck film advance lever on a film camera with the query “why is the lever not moving all the way,” which Gemini recognizes and provides some suggestions to fix. Very impressive! The only problem is that the answers it comes up with are, on the whole, hilariously bad, and Google literally highlighted the suggestion to “open the back door and gently remove the film,” which is perhaps the worst thing you can do in this situation. (If you’re not familiar with how film cameras work, opening the door in this way in anything but a totally dark room would expose your film to light, ruining any photos you’ve taken.)” Basically, if a person knows a thing or two about film photography, if they had loaded the film themselves, they probably wouldn’t need to use Google to figure out why the lever can be stuck. However, if someone is a total newbie, adding a disclaimer that back door should only be opened in complete darkness would’ve been a very good idea. But AI has no ideas at all, it doesn’t know what it’s doing, it doesn’t know anything about anything. And if even in their polished promo videos Google couldn’t avoid mistakes like this, what it’s like during real use? [Washington Post writes that it’s simply worse than a regular search.](https://www.washingtonpost.com/technology/2024/04/01/new-ai-google-search-sge/) They say that it “sometimes makes up facts, misinterprets questions and picks low-quality sources — even after nearly 11 months of public testing.” You might think that this is bad, but the real situation is actually worse, as it has been proven that Google’s AI search [“algorithms recommend scam sites that redirect visitors to unwanted Chrome extensions, fake iPhone giveaways, browser spam subscriptions, and tech support scams.”](https://www.forbes.com/sites/zakdoffman/2024/03/25/google-update-warning-chrome-windows-11-android-iphone/?sh=40d2860d1c7d) In other words, Google’s new AI search cannot be trusted. Which in my opinion defeats it’s purpose completely – you can never be completely certain that the information it gave you is accurate and harmless, you always have to verify using the regular search. Thankfully, it’s still available, [although now it’s hidden.](https://x.com/searchliaison/status/1790473952256786805) [Microsoft’s Bing Copilot has been plagued by similar issues, by the way.](https://mashable.com/article/microsoft-bing-ai-chatbot-copilot-election-misinformation-study) AI features always bring with them AI problems – bias, hallucinations, privacy issues, legal issues, disinformation etc etc.

In fact, even before Google decided to take the matters of “destroying search with AI” into their own hands, AI indirectly has been one of the main reasons search has been getting worse already. I’ll explain. A lot of websites are trying to game SEO algorithms and use AI to generate content to get them to the top of the search results list. Google, now that it’s been prioritizing slop over quality for the sake of “growth”, has been gladly presenting [the AI generated hallucinations and misinformation as their featured snippets, some examples of which include stating that “Obama is planning a martial law”.](https://theoutline.com/post/1192/google-s-featured-snippets-are-worse-than-fake-news) Google has been trying to do something about this, but [the situation is still pretty bad.](https://garymarcus.substack.com/p/what-google-should-really-be-worried) What makes the situation worse, is that some of the biggest websites on the internet like Quora have also been actively enshittified by AI, and by extension, they’ve been enshittifying Google. One of the [examples of this](https://arstechnica.com/information-technology/2023/09/can-you-melt-eggs-quoras-ai-says-yes-and-google-is-sharing-the-result/) was when Google's answer to "can you melt eggs" resulted in a "yes," because it pulled from Quora's answer generated by ChatGPT.

There’s an internet conspiracy theory called [“dead internet theory”](https://en.wikipedia.org/wiki/Dead_Internet_theory) that says that internet now consists pretty much entirely of auto-generated content and bot activity. Much of the comments, social media posts, and even the entire websites aren’t made by the real people, they’re fake. Conspiracy aspect aside, [this theory seems to be getting more real with each passing year](https://nymag.com/intelligencer/2018/12/how-much-of-the-internet-is-fake.html), and with AI the scope of the problem and pace with which it propagates is about to increase exponentially. In the beginning, the internet was vast and exciting, it was full of weird, interesting and new stuff. There were so many different websites, many with their own fun unique designs, ideas and gimmicks. It was very personal and very diverse. What it’s like now? Now most of the internet traffic goes to like 10 websites. Try to remember, when was the last time, you went on the website that isn’t a social network? The internet has gotten small and uniform, it has became predictable, controlled, locked down. And most importantly, the entirety of the internet is now poisoned by bots and auto-generated content. Needless to say, AI has been a huge contributing factor to this destruction of the web. For example, Buzzfeed, which already had a spotty track record of being an unprincipled engagement bait content farm, [has fired 180 employees and replaced them with AI.](https://english.almayadeen.net/news/technology/buzzfeed-to-use-ai-to-write-its-articles-after-firing-180-em) The problem is that [there’re already hundreds websites with AI-generated engagement bait content slop](https://www.linkedin.com/pulse/ai-generated-slop-already-clogging-arteries-web-simon-owens-obv5e/) that are manipulating Google SEO to get to the top of search result list, so I don’t think that business model of “using AI to generate garbage just to stay at the top of the trends” is sustainable, considering how heated the competition is. Especially considering that many other big info resources [like MSN (which had an AI-written travel guide recommend a food bank as one of the hottest tourists spots in Ottawa, Canada)](https://www.theverge.com/2023/8/17/23836287/microsoft-ai-recommends-ottawa-food-bank-tourist-destination) are also doing this. [The Verge has an interesting piece](https://www.theverge.com/2024/4/2/24117976/best-printer-2024-home-use-office-use-labels-school-homework) on what it’s like, trying to do a real journalism in the age of auto-generated content. AI-generated garbage has flooded the internet, now it’s everywhere. [AI-generated scammy books](https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/), and [nonsensical product descriptions](https://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/) on Amazon. [Twitter(now X) has became a “ghost town”, because of all the stupid AI bots.](https://www.abc.net.au/news/science/2024-02-28/twitter-x-fighting-bot-problem-as-ai-spam-floods-the-internet/103498070) Unfortunately, Elon Musk’s ingenious solution to make verified checkmark cost $8 [didn’t help solve the problem](https://techcrunch.com/2024/01/10/it-sure-looks-like-x-twitter-has-a-verified-bot-problem/), who would’ve thought. [Google's top result for "Johannes Vermeer" is an AI-generated version of “girl with a pearl earring”.](https://futurism.com/top-google-result-johannes-vermeer-ai-generated-knockoff) [Facebook is awash with stolen, AI-generated images that massive groups of people think are real.](https://www.404media.co/facebook-is-being-overrun-with-stolen-ai-generated-images-that-people-think-are-real/) [Disturbing, sexually suggestive AI images of children were available to purchase on the Shutterstock.](https://petapixel.com/2024/02/22/disturbing-ai-images-of-children-found-for-sale-on-shutterstock/) Image and art sharing platforms, such as Pinterest, Artstation and DeviantArt have been flooded with AI “art”. [Even YouTube isn’t safe, as it’s now full of AI-generated kids’ content.](https://www.theintrinsicperspective.com/p/here-lies-the-internet-murdered-by) Letting your kid watch even one second of those videos is equivalent of getting them lobotomized, so please, keep the kids away from youtube. While [Reddit, has been going throw its own process of enshittification](https://defector.com/the-last-page-of-the-internet), unfortunately it couldn’t escape the AI plague. I remember listening to the Verge’s podcasts (can’t find which one exactly), talking about Reddit as one of the last remaining actually useful resources on the web. If you need to find something, and you want to avoid the AI generated garbage that is now at the top of the Google’s result list, you just add “reddit” to the end of your query. Unfortunately, [AI has been poisoning Reddit as well.](https://www.404media.co/ai-is-poisoning-reddit-to-promote-products-and-game-google-with-parasite-seo/)

The new AI Overview Google Search is about to make things even worse. It’s a product, designed to cannibalize whatever left of the internet, destroy it completely. In the copyright section, I have already mentioned how this new approach to search, among other issues, also steals the traffic from the websites it pulls its information from. Why would I go to a culinary website littered with ads when I have the recipe I need neatly formatted for me by Google Overview? This feature is designed to drive people away from engaging with the original content made by real people. It injects itself in the space between users and the web, attempting to replace the internet. If the web as we currently know it isn’t truly dead yet, new Google Search might help to kill it. All with the help of AI.


### What is AI good for?

Ok, I have been talking a lot about how unreliable and almost useless AI is. Is there anything it’s actually good at? Well, there’re a few things...

AI can accidentally generate misleading and wrongful responses, either because of the hallucinations, or wrongful information in training dataset, or some other reasons. However, AI can also be used to spread disinformation intentionally. [Personalized, real-time chatbots could share conspiracy theories in increasingly credible and persuasive ways, researchers say.](https://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html) Even though LLMs’ persuasiveness can be used for good, [and help reduce people’s beliefs in conspiracy theories,](https://www.newscientist.com/article/2426618-chatbots-can-persuade-conspiracy-theorists-their-view-might-be-wrong/) they can just as easily be used to spread any kind of disinformation. Furthermore, everyone is probably aware of what immense dangers a [deepfake](https://en.wikipedia.org/wiki/Deepfake) technology can pose. For those who don’t know, this technology while being extremely easy to use, and relatively accessible, is capable of altering a video or a photo so that one person's appearance is replaced with another's in such a manner that it's almost impossible to tell that some kind of manipulation took place. It could also be used to imitate a person's voice. So far, we’re yet to see what is the actual “correct” use case for this tech, aside from silly mask filters in social media apps like snapchat or instagram, and silly memes, like videos with presidents playing video games or making photos of people into videos of them singing Dame Dame song from Yakuza video games. [There have been talks about using it to “resurrect dead popular actors” using “digital cloning”](https://www.bbc.com/future/article/20230718-how-ai-is-bringing-film-stars-back-from-the-dead#) – basically a way for studios and companies to make more money off rights to actor’s appearances that they hold the rights to. In my opinion, this is disgusting, and frankly, just dumb. However, if you think things can’t get more dystopian, wait till you hear that [there’s a booming Chinese startup that uses deepfaked AI-generated avatars of people’s dead loved ones to “help them process grief”.](https://www.technologyreview.com/2024/05/07/1092116/deepfakes-dead-chinese-business-grief/) Yeah. Well anyway, while we’re yet to see a wide use of deepfakes for “good”, or at least in any sort of professional environment, it truly shines when it’s used for all sorts of “evil” stuff. Needless to say, that’s where it’s been truly thriving. For example, using this tech, someone with malicious intent can easily create a fake but very believable audio or video of, say, a president making statements they didn't make in real life. Something, that has actually happened – [when a single person managed to run an extensive disinformation campaign meant to discourage voters ahead of primary using robocalls with deepfaked Biden voice.](https://apnews.com/article/new-hampshire-primary-biden-ai-deepfake-robocall-f3469ceb6dd613079092287994663db5)
Department of Homeland Security of United States [reports](https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf) that "the threat of Deepfakes and synthetic media (the "fake" kind of media produced with the help of deepfake technology is sometimes called "synthetic media") comes not from the technology used to create it, but from people’s natural inclination to believe what they see, and as a result deepfakes and synthetic media do not need to be particularly advanced or believable in order to be effective in spreading mis/disinformation.
Based on numerous interviews conducted with experts in the field, it is apparent that the severity and urgency of the current threat from synthetic media depends on the exposure, perspective, and position of who you ask. The spectrum of concerns ranged from “an urgent threat” to “don’t panic, just be prepared.”

Back in March 2023 a photo of swagged out Pope Francis in white puffer coat [went viral.](https://www.bostonglobe.com/2023/03/29/business/welcome-future-viral-image-pope-francis-underscores-new-reality-ai-experts-say/) It was seen and shared by millions of people, most of whom probably didn't know it was completely fake. Of course, the technology that allowed creating fake videos or images has existed for years, the problem with AI is that it made the process of creating fakes more streamlined, more accessible and so just much easier. Before the process of producing a fake was expensive and time consuming, it required employing a professional, or a whole team of professionals. Now any random person can easily create a video, image or voice recording that can fool half of the world with the help of Generative AI and social networks’ algorithms. We’ve been in the era of post-truth for a while now, however with the advent of AI and deepfakes things are about to get even worse.

AI has been actively used for all sorts of fraud. For example, [scammers last year stole approximately $11m from unsuspecting consumers(and businesses) by fabricating the voices of loved ones, doctors and attorneys using deepfakes, requesting money from their relatives and friends.](https://www.theguardian.com/business/2023/apr/09/it-sounds-like-science-fiction-but-its-not-ai-can-financially-destroy-your-business) AI has also breathed a new life in the get-rich-quick scams, as they've been [flooding the web with renewed force.](https://www.washingtonpost.com/technology/2023/05/15/can-ai-make-money-chatgpt/) There're also a lot of smaller-scale grifts riding on "AI" hype, such as [fake AI chatbot/ChatGPT apps](https://www.yahoo.com/lifestyle/fake-chatgpt-apps-raking-thousands-154124743.html) used to steal users' data, charge them with expensive subscription fees for non-existent functionality, and even install malware on their devices.

Ever since it became available, the AI tech has been used by various governments to enhance their surveillance capabilities. Recognizing faces video, automatically tracking people and objects across feeds from multiple cameras, analyzing internet traffic – now states are able to penetrate our private lives like they could never before. In fact, AI had created nothing short of a [revolution](https://www.npr.org/2023/06/13/1181868277/how-ai-is-revolutionizing-how-governments-conduct-surveillance) in how governments conduct surveillance. Perhaps in George Orwell's time, the world he described, and extent to which the government was spying on its citizens seemed grotesque, with our modern day tech, the nightmarish dystopia of 1984 might seem tame in comparison to what we have to deal with. Many states across the globe, such as [China](https://www.reuters.com/world/china/china-uses-ai-software-improve-its-surveillance-capabilities-2022-04-08/) and [Russia](https://www.wired.com/story/moscow-safe-city-ntechlab/) are already using AI to expand and enhance their control over their populations. For example, the system created by Moscow’s government, dubbed Safe City, was touted by city officials as a way to streamline its public safety systems. In recent years, however, its 217,000 surveillance cameras, designed to catch criminals and terrorists, have been turned against protestors, political rivals, and journalists. Utilizing one of the most advanced facial recognition systems – Sfera – Russian government was able to take totalitarian control to a completely new level.

AI use in surveillance poses unprecedented danger to civil liberties around the world. In fact, [the vast majority of computer vision research leads to technology that surveils human beings.](https://www.404media.co/how-the-surveillance-ai-pipeline-literally-objectifies-human-beings/) Signal's CEO Meredith Whittaker goes as far as calling [AI a fundamentally "surveillance technology."](https://techcrunch.com/2023/09/25/signals-meredith-whittaker-ai-is-fundamentally-a-surveillance-technology)

It's not hard to imagine what kind of impact a tech so powerful could have on military industry. [Forbes reports](https://www.forbes.com/sites/cognitiveworld/2019/01/14/the-weaponization-of-artificial-intelligence/?sh=6f7ce9243686) that "The reality today is that artificial intelligence is leading us toward a new algorithmic warfare battlefield that has no boundaries or borders, may or may not have humans involved, and will be impossible to understand and perhaps control across the human ecosystem in cyberspace, geospace, and space (CGS)." Very little is known about the weapons that are being developed with the use of AI as this kind of research is rarely made open to public. However, I think it's fair to say that this kind of weaponry has the potential to be immensely dangerous. One of the example of AI weapons is Autonomous Weapon Systems (AWS). As Birgitta Dresp-Langley, an AI researcher [writes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10030838/): "The potential consequences of a deployment of AWS for citizen stakeholders are incommensurable..."

One of the more prominent examples of military uses of AI is the “Lavender” system used by Israeli army to mark tens of thousands of Gazans as suspects for assassination, as [reported by +972 and Local Call,](https://www.972mag.com/lavender-ai-israeli-army-gaza/) whom I will loosely quote below. This AI targeting system is used with little human oversight and a permissive policy for casualties. Additional automated systems, including one called “Where’s Daddy?”, were used specifically to track the targeted individuals and carry out bombings when they had entered their family’s residences.


### AI is hurting our planet

[The MIT Technology Review reported](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/) that training just one AI model can emit more than 626,00 pounds of carbon dioxide equivalent – which is nearly five times the lifetime emissions of an average American car.
Earth.org [writes](https://earth.org/the-green-dilemma-can-ai-fulfil-its-potential-without-harming-the-environment), that “as datasets and models become more complex, the energy needed to train and run AI models becomes enormous. This increase in energy use directly affects greenhouse gas emissions, aggravating climate change.” Microsoft’s carbon emissions have jumped 30% as company seemingly has abandoned its goal to become carbon negative by 2030 – [Bloomberg reports.](https://www.bloomberg.com/news/articles/2024-05-15/microsoft-s-ai-investment-imperils-climate-goal-as-emissions-jump-30) They also write that “tech giants' electricity consumption is growing rapidly and rivaling that of small European countries”, showing that Google as well as Microsoft now consume more electricity than the entire country of Slovenia. Meanwhile, [Arm says that AI computing is on pace to consume more energy than India by 2030, if the current trends continue.](https://www.bloomberg.com/news/articles/2024-04-17/ai-computing-is-on-pace-to-consume-more-energy-than-india-arm-says)

Training AI also requires immense amounts of water for cooling the GPUs. [Microsoft’s water usage for cooling tech has shot up over a third and could fill 2,500 Olympic-sized pools.](https://www.standard.co.uk/news/tech/ai-chatgpt-water-power-usage-b1106592.html#) [Just last year Microsoft has drawn a lot of criticism for using a lot of water during draught and putting strain on water systems across the US.](https://futurism.com/critics-microsoft-water-train-ai-drought) In my opinion, deservedly so. “Microsoft increased worldwide water consumption by a whopping 34 percent — up to almost 1.7 billion gallons annually” in 2022. Running AI models also requires a lot of water as well, as, for example, [“ChatGPT gulps 500ml of water for every 5-50 prompts it answers.”](https://www.businesstoday.in/technology/news/story/microsofts-water-usage-surges-by-thousands-of-gallons-after-the-launch-of-chatgpt-study-397951-2023-09-11)

This begs the question – is AI worth destroying the Earth for? I know my answer.


### Wrapping up

Unfortunately, there’re so many important issues I was unable to dissect here, like how AI hurts [science](https://www.404media.co/chatgpt-looms-over-the-peer-review-crisis/) and [education](https://nepc.colorado.edu/publication/ai), for example, or how it has been used for creative new ways to [harass girls](https://www.eviemagazine.com/post/girl-14-commits-suicide-boys-shared-fake-nude-photo-suicide-squad) and [oppress women,](https://www.captechu.edu/blog/artificial-intelligence-and-its-unique-threat-women#:~:text=Perhaps%20the%20most%20significant%20threat,in%20compromising%20or%20falsified%20situations.) for [sextortion](https://www.forbes.com/sites/forbestechcouncil/2024/02/20/the-evolution-of-sextortion-attacks-how-generative-ai-is-taking-a-front-seat/?sh=4fbb21d620fc), and, of course, [for porn.](https://www.euronews.com/next/2023/10/20/generative-ai-fueling-spread-of-deepfake-pornography-across-the-internet) Each of these topics is very broad and requires a careful and detailed analysis, but this article’s already gotten much bigger than I originally planned, and honestly I’m just too tired. Perhaps, I will make separate posts just about those problems some time in the future. But I want to stress out, that these issues are equally as significant as everything I talked about here, so please click on the reference links I provided above, when you have a moment.

So what’s the current state of AI? It’s a technology that is created with unethically and probably illegally sourced data, which is processed using slave labor, that can propagate harmful biases and spread disinformation, and while it’s almost useless in most of the _”good”_ real-life use cases, it shines when used for crime, surveillance or porn. All the while, billions of dollars are being poured into this thing with no clear path to improvement and profitability making it just a one huge financial bubble. AI’s been destroying the internet and making the beloved services and platforms unusable. Did I miss anything? The worst thing about this is that we’ve been through the similar hype-bubble cycle just a few years prior with Web3/crypto/NFTs.

I knew that Apple Vision Pro was going to flop. but I kept my incredible predictions to myself, because I didn’t want to be wrong. But what’s the lesson here? I guess, the lesson is that I should no longer keep my very precise and very correct valuable predictions hidden from the world anymore. Because you can’t even imagine how tired I am... I’m tired from being right all the time, again and again.

So what does “faith in humanity” have to do with all this? Something I find truly bizarre about AI hype, is witnessing how people how’re supposed to be smart and responsible are making the most stupid and incredibly reckless decisions, how people, who should’ve known better, people who had our trust just show time and time again that they just don’t know what they’re doing. CEOs, VCs, governement officials, researchers – we put our trust in these people, we give them money, we listen to them, and what they did is they abused their power and let us all down. It would’ve been funny, if it weren’t so sad. What’s even worse is that this is the unfunny joke that’s been repeated twice. We just got out of the Web3 hype-cycle, and dove headfirst into the AI hype-cycle. This’s what kind of thinking we’re dealing with here: [“Microsoft president Brad Smith says the good AI can do for the world will outweigh its environmental impact.”](https://www.bloomberg.com/news/articles/2024-05-15/microsoft-s-ai-investment-imperils-climate-goal-as-emissions-jump-30) I’m not going to mince my words here and just going to put it plainly – this’s incredibly stupid, almost parody territory. No, the “good” LLMs can do in their current state don’t even outweigh the $20 costs of monthly subscription for majority of world’s population.

[Acknowledgements and recommendations]
