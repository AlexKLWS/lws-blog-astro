# AI vs Silicon Valley
## A close look at the current "next big thing" and culture that surrounds it

"I've lost all faith in humanity.” This is a phrase that I’ve seen being thrown around a lot. Whenever I see it used by grumpy middle-aged men with Ron Swanson profile pictures commenting on Reddit under a post about people camping outside an Apple store on the day a new iPhone dropped I can't help but feel a bit annoyed. If this is all it takes to make you give up on humanity wait till you hear about something called "war". Of course I understand that whoever uses this phrase usually doesn't _really_ mean it. I remember that back in 2010 seeing a photo of young fans going crazy for Justin Bieber was enough for Ron-Swanson-profile-pic internet commentator to get all upset. Me, on the other hand, I try to stay positive, even when I witness such serious transgressions as people liking musicians that I don't. However, I have to admit that in recent years my faith in humanity has been tested on many occasions. Not in an exaggerated ironic way, but in a serious way. Even though I won't be talking about something as heavy as wars or human rights violations, the topic of this post is still pretty serious. I'd like to talk about one of the hottest new fads – something called "AI". Please stick with me till the end, you’ll understand what faith in humanity has to do with it.

This post is going to be very very critical of AI, so before I’m accused of being a luddite ([not that there’s anything wrong with being one](https://headgum.com/factually-with-adam-conover/why-big-tech-is-ruining-our-lives-with-brian-merchant)), I want to make it clear that I’m not _against_ AI. I think, it is truly one of the greatest technological breakthroughs in recent human history. It has a potential to improve many people's lives and maybe even revolutionize entire industries. In my opinion, it’s not a solution to all world’s problems (as many “experts” are suggesting), but a _tool_ that can be immensely useful _in certain contexts_. However, like any other new technology AI is marred by very serious problems. It’s crucial that these problems are properly addressed even if this comes at a cost of slowing down. Frankly, I believe that the antagonism between “progress” and “solving issues this progress brings” is often greatly exaggerated and used in bad faith. Both can be done at the same time. I’ll leave it at this for now as I plan go deeper into this topic in part 2. Either way, the problems AI brings are very serious, as will become clear from this post, and should be addressed as quickly as possible. Unfortunately, this is extremely difficult due to the impenetrable bubble of hype and disinformation that surrounds this tech. Honestly, I think it's almost impossible for the average person to even understand how the AI _actually_ works due to the fact that so many news articles, social media posts, even scientific papers on the topic are full of complete nonsense and outright lies. In truth, even the term "AI" itself is very misleading, as even the craziest most devoted AI cultists don’t really believe that whatever tech we have right now can be called a true “Artificial Intelligence”, since even the most advanced LLMs are severely lacking in the “intelligence” department. Some of the least critical researchers talk about systems we have right now having only little glimpses of “intelligence”, or [“sparks of AGI” (Artificial General Intelligence)](https://arxiv.org/abs/2303.12712) as they put it themselves. Meanwhile, other researchers even argue (very convincingly) that [AI isn’t “artificial” nor is it truly “intelligent”.](https://www.wired.com/story/researcher-says-ai-not-artificial-intelligent/) In fact, despite many claims from some industry experts and even some prominent scientists that AI is actually intelligent, or sentient or maybe even has a soul, there’s an overwhelming evidence indicating that has none of these things, and in truth, isn’t all that “smart.” This topic is very broad so I want get too deep into it in this post. Either way, while the fact whether the current AI is a “real AI” is still debatable, the dangers posed by this technology are undoubtedly very real. Many experts, researchers and organizations are fighting day and night trying to educate people about this technology and bring public’s and governments’ attention to these problems. Unfortunately this fight against disinformation is still an uphill battle because so much of it is actually being pushed by biggest multibillion dollar corporations (such as Google and Microsoft) and richest, most powerful men in the world (such as Elon Musk and Peter Thiel). They use their almost infinite resources and influence to shift public's attention from real, tangible problems of the present to some very vague "AI potential” of the future. This in itself is a very serious issue. If the real problems posed by AI aren't solved soon, and we as a society continue on the current trajectory, all of us might end up in a very dark territory. With this post, I’m hoping to make my small contribution in this fight against disinformation.

I don’t think I’m treading any new grounds with this article, pretty much all information and ideas you can find here are taken from somewhere else. My intention is to have all this information aggregated in one place, have this post as a single point of reference for everything related to the dangers and problems AI poses. I also want it to be a good starting point for continuing research for anyone who would like to dive deeper into the topic.

In part 1 (this post), I will go through some of the most serious problems posed by AI technology in detail. For each of these problems I will try to provide the analysis, as well as my personal opinions.

In part 2 I will take a deep dive into the crazy polemic of the most prominent AI hype-men. I will explore the thought process of those people, their philosophy and goals. In the end, I will share my thoughts on the future of Silicon Valley and humanity’s approach to technological advancement in general.

I have been writing this post for almost an entire year. I guess I shot myself in the foot trying to write a comprehensive article about a constantly changing and evolving topic. To be quite honest, this has been a very exhausting journey, so I don’t know if the part 2 will come out any time soon, if ever. I don’t want to deal with moving targets anymore, so if anything, I want to wait until this hype-bubble bursts before starting on part 2.


### AI "sweatshops" in the Global South and around the world

At the heart of what most people currently call “AI” lies the technology called [”Artificial Neural Network”](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)). An Artificial Neural Network (usually called simply Neural Network or NN) is, roughly speaking, a program which design is *inspired* by the anatomy of human brain. Before this program can be used for anything, it needs to go through the process called “training” or “learning”. You might’ve seen the term Machine Learning or ML – this is what this field was called before it was hijacked by marketing teams pushing for calling Neural Networks AI for the sake of hype. In fact, calling the process “training” or “learning” is a bit misleading as well. “Function fitting” would’ve been more...uhm well fitting here. Either way, this process of training is integral to the development of pretty much all modern AI tech. Most-powerful NNs, such as ChatGPT, Gemini (ex-Bard), Midjourney or SnapML, require LOTS and LOTS of data to be trained. This data needs to be somehow obtained, and then processed. I'll talk about issues with sourcing the data later, for now I would like to focus on processing or preparing data for training. What does this usually entail? Well, if we are building a NN that’s capable of detecting whether a photo has a bicycle, for example, we would need to:
- Make sure that each photo is the same size in pixels
- Each photo can be read by the program without any issues, has correct file format
- Most importantly, each photo must have an arbitrary "yes" (meaning there's a bicycle in the photo) or "no" (no bicycle) label associated with it
- Perhaps there're some other considerations that I haven't thought about

Even though some of this work can be automated, the most crucial part – labeling the photos – has to be performed manually by people. And this is what processing or preparing the data essentially means – sorting, ordering, filtering, resizing, and most importantly, labeling each and every individual piece of data used for training. Unfortunately, there's no magical solution to this a real life human being has to sit down and sift through the photos carefully marking them one by one. Imagine going through the thousands of photos like this. This is an incredibly difficult, stupefyingly tedious and above all very time consuming process. How did the big corporations solve this problem? Pretty much like how they solved all their problems with the dangerous, expensive and equally as tedious industrial production processes – [they outsourced (most of) this work to the Global South.](https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/) TIME reported that [OpenAI used Kenyan workers making $2 an hour to filter traumatic content from ChatGPT.](https://time.com/6247678/openai-chatgpt-kenya-workers/) I will be loosely quoting the article in a paragraph below, please give original a read if you'd like to get a better picture.

In order to build an "AI" that could detect toxic language like hate speech and to help remove it from their platforms OpenAI needed to train it on labeled examples of violence, hate speech, and sexual abuse. To that end they engaged with Sama, a San Francisco-based firm that employs workers in Kenya, Uganda and India to label data for Silicon Valley clients like Google, Meta and Microsoft. Sama markets itself as an “ethical AI” company and claims to have helped lift more than [50,000](https://www.sama.com/blog/we-are-a-b-corp/) people out of poverty. However, despite their integral role in building ChatGPT, the workers at Sama faced grueling conditions and low pay. An agent working nine-hour shifts could expect to take home a total of at least $1.32 per hour after tax, rising to as high as $1.44 per hour if they exceeded all their targets. There is no universal minimum wage in Kenya, but at the time these workers were employed the minimum wage for a receptionist in Nairobi was $1.52 per hour. To get the training data processed and labeled, OpenAI sent tens of thousands of snippets of text to Sama, beginning in November 2021. Much of that text appeared to have been pulled from the darkest recesses of the internet. Some of it described situations in graphic detail like child sexual abuse, bestiality, murder, suicide, torture, self harm, and incest. Needless to say going through this data is a very hard ordeal. Some workers described being mentally scarred by the work. Some even went as far as calling it a torture. And of course, OpenAI isn’t the only company that worked with Sama. TIME also published another relevant story titled ["Inside Facebook’s African Sweatshop."](https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/) The investigation detailed how Sama employed content moderators for Facebook, whose jobs involved viewing images and videos of executions, rape and child abuse for as little as $1.50 per hour.

Considering that OpenAI is [valued above $29 billion](https://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279), while Microsoft alone [has already invested in it a whopping $10 billion](https://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in-openai#xj4y7vzkg), it really does make you think – why can’t such a weathy company create better conditions for workers responsible for one of the most difficult and most important parts of their core product? 

Big corporations aren’t the only ones trying to outsource this problem. Small start-ups also often do the same. Some get pretty creative with it, like for example [Metroc, a Finnish start-up which is using prison labor to help it train AI.](https://www.wired.com/story/prisoners-training-ai-finland/) The reason they has to resort to such a solution is that not many people in the Global South speak Finnish, so they had to look for cheap easily exploitable workers within their country’s borders.

“The changes coming are unstoppable,” [Sam Altman, the CEO of OpenAI said.](https://www.cnbc.com/2021/03/17/openais-altman-ai-will-make-wealth-to-pay-all-adults-13500-a-year.html) “If we embrace them and plan for them, we can use them to create a much fairer, happier, and more prosperous society. The future can be almost unimaginably great.” Knowing what the development process of OpenAI’s products involves, I can’t help but wonder: who is he talking about when he talks about “society”? Definitely not the people working on building this technology for him. In my opinion, the AI that is supposed to "create a much fairer, happier, and more prosperous society" can’t require hundreds of people slaving away in a sweatshop-like environments in order to work. What we have right now feels less like a technology of the future and more like a very impressive [Mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk) that on the inside is actually a primitive mechanism being operated by slave children pushing pedals. With such incredible amounts of money thrown at this thing, we can do so much better. 

### AI art is ethically and legally dubious 

Unfortunately, the process of obtaining data required for training NNs is also often problematic. Let's take a look at "generative AIs" for example. A “generative AI” is a sort of AI that can generate different types of content (such as images, videos, music, etc) after being trained on many examples of this kind of content. For example, in order to produce an image that looks like an artistic drawing, a generative AI has to be trained on millions, or even billions of real artists' works. In order to obtain those works, the company working on AI has to [scrape (scraping is the process of extracting data from websites)](https://en.wikipedia.org/wiki/Web_scraping) the web downloading images from resources such as DeviantArt and ArtStation, and feed them to the NN for training. The issue with this, is that no one actually bothered to ask artists whether they want for their works to be used in such way. Another problem is that the legality of this process is also very dubious, considering that a lot of art used for training is protected under copyright. When asked by [Forbes](https://www.forbes.com/sites/robsalkowitz/2022/09/16/midjourney-founder-david-holz-on-the-impact-of-ai-on-art-imagination-and-the-creative-economy/?sh=33dbd4ca2d2b) in a recent interview if the Midjourney seeks out permission from the artists whose images they’ve used to train their program, company's CEO David Holz simply said there was no feasible way to do so. “No,” Holz underscored. “There isn’t really a way to get a hundred million images and know where they’re coming from. It would be cool if images had metadata embedded in them about the copyright owner or something. But that’s not a thing; there’s not a registry. There’s no way to find a picture on the Internet, and then automatically trace it to an owner and then have any way of doing anything to authenticate it.” 
Other companies have made similar statements. [“Because copyright today covers virtually every sort of human expression – including blogposts, photographs, forum posts, scraps of software code, and government documents – it would be impossible to train today’s leading AI models without using copyrighted materials,” said OpenAI in its submission, first reported by the Telegraph.](https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai)
In my opinion if you deiced to become obscenely rich through means that are both unethical and illegal, you should probably reconsider. For example, if there’s no ethical and legal way to steal other people’s money from a bank then you shouldn’t do it. Even if stealing money from a bank was somehow legal, an ethical problem would still remain, as by stealing other people’s money you’re hurting them directly. Does this make sense? I think the same logic applies here, and we should find a better way to go about things. But maybe there’s some special Silicon Valley tech bro logic involved in this that I don’t yet comprehend? Anyhow, this situation has created, as Kotaku had written, [“an ethical and copyright black hole, where everyone from artists to lawyers to engineers has very strong opinions on what this all means, for their jobs and for the nature of art itself.”](https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060) In my opinion there can be no debate on the ethics of this approach – using other people’s work without their consent is bad no matter what angle you look at it from. The legality of this is a whole another issue, however. Naturally, after seeing how their works have been used by the AI without their consent to generate profit for 3rd parties many people went to courts to fight for their rights. [Stability AI, Midjourney and DeviantArt are all currently being sued by artists for copyright infringement](https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart). The AI corporations/start-ups are trying to stall the lawsuits in any way they can. Thankfully, their attempts haven’t been very successful so far. For example  relatively recent ( and in my opinion nonsensical) [arguments from Midjourney’s lawyers that “the suit is intended to suppress its free speech” has been dismissed by the court.](https://www.hollywoodreporter.com/business/business-news/artist-lawsuit-ai-midjourney-art-1235821096/) This kind of rhetoric looks desperate, if you ask me. But why the desperation? Well, the first reason is, as Axios puts it, that [“looming \[court\] fights over copyright in AI are likely to set the new technology's course in 2024 faster than legislation or regulation.”](https://www.axios.com/2024/01/02/copyright-law-violation-artificial-intelligence-courts) The representatives of the top AI companies and “leading experts” have been doing all in their power to sweet-talk the governments to not regulate AI. How they do it is something I will talk about later, but answer to the question “why” is that the regulations will hurt corporate profits, obviously. Because of lobbying, and other kinds of manipulations, many of the world’s governments have been very slow to address the dangers posed by AI in any meaningful way, while these dangers have been growing more significant at an increasing pace. However, a court decision might be able to rein in AI before governments will have a chance to, and this is something AI companies want to avoid. And so far it doesn’t look like AI companies have a good chance of winning their cases. Let’s take a look at probably the most important lawsuit of them all – OpenAI v New York Times. Ars Technica has a [very good article](https://arstechnica.com/tech-policy/2024/02/why-the-new-york-times-might-win-its-copyright-lawsuit-against-openai/) exploring this lawsuit. In it, the authors go through the history of similar cases and outline what were the decisive factors in each of them. Often, the company’s accused of copyright breach are trying to claim fair use, as does OpenAI. The article states that the “courts are supposed to consider four factors in fair use cases, but two of these factors tend to be the most important. One is the nature of the use. A use is more likely to be fair if it is “transformative”—that is, if the new use has a dramatically different purpose and character from the original ... The other key factor is how a use impacts the market for the original work.” The article analyzes the Authors Guild v. Google lawsuit in particular. Back in 2004 Google has announced its new project – Google Books. The idea was to scan millions of books and make them available in their search engine. Authors and publishers argued that doing so was violating copyright laws and took their argument to court. However, Google was able to win the lawsuit, primarily because, first, the use of the original material was transformative – the users were able to search for text from a particular book using Google Search, but they were unable to read the book in its entirety using this service – and second, because it was having a positive impact on the book market – with the help of Google Books people were able to find new books that interested them. Imagine doing a research. You type in information that you’re looking for in Google’s search bar and at the top of the search results you see Google Books webpage. On it you can view a few pages from the book, and see whether it has something you’re looking for, or not. If you find this book useful, you would buy it, naturally. In the end, this is a win-win-win situation – it’s a win for the publisher/author, a win for the user and a win for Google. Now let’s take a look at ChatGPT, how does it fare in regard to the 2 afore mentioned fair use factors? First, the ChatGPT’s use isn’t transformative. NYT has compiled a very strong case, they were able to provide more than 100 examples which demonstrate that ChatGPT often quotes NYT articles verbatim several paragraphs at a time. Even though OpenAI claims that this is just a bug, and something that they are working on eliminating, it is clear that their position is very shaky here. Even if they will be able to eliminate this bug completely via fine tuning, the problem here is that this demonstrates that system is _capable_ of regurgitating copyrighted content. In the lawsuit documents we can see that ChatGPT outputs copyrighted data when prompted to do so directly. However, there’s also a chance that an AI can output big chunks of copyrighted content during a regular conversation, without citing the source. LLMs actually struggle a lot with providing the sources of the information they were trined on. As this blog post from Microsoft states, [“ChatGPT will make attempts to provide sources for its content, but its primary function is to reproduce patterns in text, not to actively consult sources to provide accurate information.”](https://www.microsoft.com/en-us/microsoft-365-life-hacks/writing/using-chatgpt-for-source-citation) Also, fun fact: AI reproduces creative works as part of the training process. OpenAI has noted this in [court](https://storage.courtlistener.com/recap/gov.uscourts.cand.415174/gov.uscourts.cand.415174.32.0.pdf) and other official filings. The bigger problem here, however, and this also ties in with the second factor, is that ChatGPT most likely will actually compete with NYT, as well as many other news outlets. [Microsoft has plugged ChatGPT into its Bing search,](https://www.microsoft.com/en-us/bing?ep=278&form=MA13LT&es=31) which signals to us that one of the potential use cases the company sees for the AI systems is to be a go-to single all encompassing source of information. In other words, they want for AI to replace the internet. Instead of searching the web, the users would prompt AI and get answers without navigating away from the Bing page. Microsoft aren’t the only ones to arrive at this idea. The Browser Company (a company behind Arc browser) was quick to jump on the AI hype train, and has released an [Arc Search app.](https://arc.net/blog/arc-search) This app is “focused on getting you to an answer faster, without all the clutter of the internet”, its promise is that it “browses for you” – you type in your question, it gives you an AI generated answer. This kind of thing is problematic from multiple angles, but what’s important for now is that this app competes with websites it takes its information from. Google [has similar plans](https://search.google/features/search-labs/) for its Bard/Gemini/Whatever-it-will-be-called-next AI, and there’re a few other examples. How do these systems compete with NYT? Now if people use Bing search engine, they might get NYT as a first search suggestion, find an answer there, and bring in some revenue via ad impressions/clicks or maybe even potentially become  their customer (by purchasing a subscription, for example). With AI system being pushed as a replacement for search, the users will get the information the look for right in the AI chat box, and chances that they will navigate to NYT website from there are very slim. By substituting the internet with AI, you substitute NYT.com with AI, I think this is obvious. Actually, legal and ethical problems aside, this kind of systems are unsustainable, in my opinion. Imagine you own a small very specialized website containing very valuable information on a certain topic. This information needs to be updated regularly, so to keep the whole thing running you make some income from ads + you have a link to your Patreon (or other monetization platform) on your website. Imagine an AI company that also regularly scrapes all the information from your website, and uses it to train their model. Then let’s imagine that this company would have an AI that is able to provide the same information you have on your website soon after you post it. If an AI system is injected into people’s information-search-flow as the first and only step, as is the intention with Bing AI, people would be finding the information you provide via an AI and considering how it struggles with citations, they would never discover your website. Naturally, because of this, the income you were getting to support your project would start to dwindle. You might wrap up your project, or change the format of your content delivery (start publishing it in a magazine, for example) and because of this the value of AI for people who were using it to get the info you were providing would fall. As AI hurts organizations and individuals by extracting information they provide and then presenting it as its own, these organizations or individuals would either go out of business, or try to become unreachable by scraping. Either way,  by trying to replace the internet, AI companies might end up shooting themselves in the foot. They are aware of this, and use various approaches to try to deal with this problem. Google, for example, has decided to resort to an extreme solution – it plans to strike a deal with organizations whose data it’ll use to train their AI and actually pay them! Well, there’s only one such organization for now – [Google’s made a deal with Reddit to use their data to train AI.](https://www.theverge.com/2024/2/22/24080165/google-reddit-ai-training-data) The Reddit users, who are actually the ones producing content for the platform and thus bringing any value to the company, weren’t asked. Anyway, this [isn’t the first unpopular decision Reddit’s leadership has made.](https://www.techdirt.com/2023/06/16/reddit-ceo-triples-down-insults-protesters-whines-about-not-making-enough-money-from-reddit-users/) OpenAI, on the other hand, has been scraping the content that is supposed to be unscrapable. Or at least, it seems that way, as there has been [evidence that their web crawler ignores Robots.txt.](https://news.ycombinator.com/item?id=40001971) Getting back to the NYT lawsuit, even though they had prepared a very strong case, it’s still impossible to predict which way will the events go. Either way, I will be monitoring this situation, and I recommend you do the same. Probably the second most important lawsuit that has a potential to define the future of the technology is Getty v Stability AI, so I recommend watching this one closely as well.

Here I want to make a very important note. Copyright is a very nuanced topic, and even if some valid arguments can be made against it, it cannot be simply abolished overnight. Some of the AI supporters holding right-wing libertarian political views have been touting AI as a tool that could potentially dismantle the copyright system. The problem, is that if went to avoid a catastrophe, there needs to be an alternative better system in place before any dismantling can happen. Otherwise, this would be disaster, as entire industries and millions of people would lose means to support their livelihoods. As Dominic Young, the CEO of Axate writes in his [report for the House of Lords Communications and Digital Select Committee](https://committees.parliament.uk/writtenevidence/128368/pdf/) “Copyright is a precious & powerful economic & cultural engine. When it’s undermined, as it was for the benefit of the big platforms…it destroys value & creates monopolies. We must not repeat that historic mistake when it comes to AI.”

Meanwhile, some companies like Adobe managed to sidestep the legal issues altogether, at least for now. But even well-established mega-corporations, have to take a few “ethical shortcuts” to be able to train a generative AI that’s capable of competing with Midjourney and OpenAI’s DALL-E. Despite Adobe boasting that their ["Firefly AI"](https://www.creativebloq.com/features/everything-you-need-to-know-about-adobe-firefly) is the most "ethical" generative AI, people are very [skeptical](https://newart.press/p/is-adobe-fireflys-ethical-dataset) especially considering Adobe's [very unethical track record](https://en.wikipedia.org/wiki/Adobe_Inc.#Criticisms). Firefly’s datasets are sourced primarily from Public Domain resources as well as their own service – Adobe Stock. Basically, Adobe is leveraging its monopolistic position on the graphic design market to obtain 100% legal images for training their model. "100% legal" doesn't mean explicit consent, however. Many artists don't even know their works are being used to train an AI. Of course whenever someone submits their work to the Adobe Stock they give Adobe legal rights to do anything the company wants with it, including AI training. It's right there, hidden away in [Contributor Agreement Additional Terms](https://wwwimages2.adobe.com/content/dam/cc/en/legal/servicetou/Adobe_Stock_Contributor_Agreement_Addl_Terms_en_US_20220415.pdf) as "developing new features and services" – the definition so vague it's pretty much the equivalent of fine print. And of course, as of now there's [no way to opt out](https://helpx.adobe.com/ca/stock/contributor/help/firefly-faq-for-adobe-stock-contributors.html) of data set training for content submitted to Stock. However, in April 2024 it became known that Adobe has been [actually using Midjoureny-produced images to train Firefly.](https://www.bloomberg.com/news/articles/2024-04-12/adobe-s-ai-firefly-used-ai-generated-images-from-rivals-for-training) I think at this point, we can definitely say that Firefly AI is anything but “ethical”.

Something that I think is very important to mention here, is that there’s an opinion that the most important thing about any AI model is their dataset. Or, in other words, AI models are their dataset. [OpenAI engineer James Betker writes](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/), that “...trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point... [This] implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.
Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.” This is a very clever observation, and it does make a lot of sense in my opinion. However, this fact has some very serious implications to how AI can be not a copyright infringement, considering models are their dataset.

Ok the use of copyrighted work by AI is problematic, but what is the legal status of the works produced by an AI? [United States District Court has ruled that no work produced by AI can be copyrighted.](https://www.theverge.com/2023/8/19/23838458/ai-generated-art-no-copyright-district-court) In a way this implies that you can’t make much of a money being just “an-AI artist.” Unless you make the significant part of the work yourself, your AI-produced content can’t be considered as something that you made yourself. As it should be, in my opinion. This makes AI seem more like just a one more tool in artists’ repertoire, and less like an all-encompassing complete solution to art, and a total replacement for artists. I will get back to this point later, but for now...

### Some business owners are buying into hype and replacing or threatening to replace creative workers with AI

While using artists works without their permissions is a very serious problem in itself, it leads to other problems, some of which are even more serious. Many artists who already struggle to make ends meet feel that their livelihoods are being threatened by AIs that, as every “tech genius” boasts, supposed to replace them. Here’s the future that multibillion dollar corporations envision for us: instead of paying an artist to commission a work from them, people are supposed to be paying the tech corporations so that they can use an AI to produce content generated from that artist's old works, all without paying the artist themself or even acknowledging them. [AI is already taking video game illustrators’ jobs in China](https://restofworld.org/2023/ai-image-china-video-game-layoffs/), and  [studios from other parts of the world are following suit.](https://kotaku.com/ai-art-layoff-video-game-studio-pc-midjourney-aigc-1850489333) Disney has [created a “task force to explore AI and cut costs.”](https://www.reuters.com/technology/disney-creates-task-force-explore-ai-cut-costs-sources-2023-08-08/) The AI adoption by [game developers (“31% of game developers already use generative AI”)](https://www.pcgamer.com/31-of-game-developers-already-use-generative-ai/), [movie companies](https://www.forbes.com/sites/neilsahota/2024/03/08/the-ai-takeover-in-cinema-how-movie-studios-use-artificial-intelligence/?sh=1ab4d924a3fd) and other creative studios has been very rapid and extensive. Returning back to Disney for a second here, the mega-corporation has already [used the technology in the production of season 2 of Loki TV show](https://www.theverge.com/2023/10/9/23909529/disney-marvel-loki-generative-ai-poster-backlash-season-2) and [in the opening credits of Secret Invasion TV show.](https://mashable.com/article/secret-invasion-ai-opening-credits) Taking all this into consideration, I think it’s safe to say that the artists' fears are not unfounded.

Midjourney, as well as some other tools, even allow [using artists' names](https://decentralizedcreator.com/list-of-artists-supported-by-midjourney/) as prompts to generate content in their style. The list of supported artists' names is relatively short, as of the time of this post, but is supposed to be expanded in the future. 

According to the [SoA survey](https://www2.societyofauthors.org/2024/04/11/soa-survey-reveals-a-third-of-translators-and-quarter-of-illustrators-losing-work-to-ai/) about a quarter of illustrators (26%) have already lost their jobs due to AI, and over a third of them (37%) had reported that income from their work has decreased in value because of generative AI.

The AI-replacement doesn't only apply to visual arts, by the way. Today, voice actors sitting down to record for games companies are forced to begin each session with "My name is ______ and I hereby grant irrevocable permission to train an AI with my voice and use it any way you see fit." Some game developers are already completely replacing voice actors with AI. For example, [Embark Studios, the developers of the one of the biggest upcoming online FPS games “The Finals”, have been relying entirely on AI for its voicework.](https://www.forbes.com/sites/paultassi/2023/10/29/the-finals-is-using-ai-voicework-and-it-is-bad/?sh=54c98b491d34) Many voice actors and advocacy organizations, [report](https://www.vice.com/en/article/5d37za/voice-actors-sign-away-rights-to-artificial-intelligence) that contracts including language around synthetic voices are now becoming more prevalent. [AI generated hip-hop has been flooding the internet](https://rollingstoneindia.com/just-because-ai-generated-rap-songs-go-viral-doesnt-mean-theyre-good/), and smaller artists are worried that there's yet another obstacle on their path to success.

Hollywood screenwriters jobs have also been [under attack from AI.](https://www.wired.co.uk/article/hollywood-screenwriters-artificial-intelligence-guardrails) Even film actors aren't safe from AI. [Hollywood studios want to scan background actors for one day's pay, and then use likenesses for 'rest of eternity,' SAG claims.](https://people.com/sag-claims-studios-want-to-scan-background-actors-use-likenesses-for-free-7561285)

Taking all this into account, I think it’s fair to say that there’s a concerning trend in the creative industry to replace real people with AI. The AI doesn’t care, but people have to suffer terrible consequences, especially those were in a precarious position already. To be completely honest, I find this situation sickening. There once was hope, that most difficult, boring and repetitive tasks will be taken on by machines, so that we, the humans, would have more free time to spend writing poems, painting, composing music – in other words creating art and expressing ourselves. Doing what we love, doing what makes us human. The promise of technology was to free us from hard and mundane labour, and allow us to fulfill our true potentials. I think there's some very dark irony in the fact that creative professions are among the first to come under attack by the AI companies. I don’t believe that this attack will be successful, partially, because art is not something that can be “solved” by AI, and because people trying to replace artists with AI misunderstand what art is at the fundamental level, but also partially due to the reasons I will get into later. 

David Holz (CEO of Midjourney) [says that](https://www.forbes.com/sites/robsalkowitz/2022/09/16/midjourney-founder-david-holz-on-the-impact-of-ai-on-art-imagination-and-the-creative-economy/?sh=33dbd4ca2d2b) "Midjourney is designed to unlock the creativity of ordinary people by giving them tools to make beautiful pictures just by describing them." This means that before AI, someone who struggles with drawing could use *words* to describe to an artist what they want to be drawn, pay the artist money for it, and then get an original (to a certain extent) work created by a human, who probably enjoyed the process of creating the work, and maybe even poured their heart and soul into it. However, what GenAI companies like Midjourney suggest, is that with their technology a person who struggles with drawing could use *words* to describe what they want to be drawn to an AI and pay a corporation who owns it to get a tasteless amalgamation of regurgitated works used without their creators' permissions. I don’t know about you, but to me this proposition just seems worse. And I think it goes without saying that feeding someone’s ideas into AI prompt text input has very little to do with “unlocking creativity”.

All in all, in my opinion there might be a huge correction in the creative industry away from AI some time in the future. The tech will definitely stay, but its usage will settle in a less prominent capacity than what AI hype-men preach. However, this won’t come painlessly as damage is already being done. And unfortunately, creative professions aren’t the only ones that are being replaced with AI.


### Some business owners are buying into hype and replacing or threatening to replace ALL workers with AI

It seems that workers in almost all professions are under the risk of being replaced by AI as well. In March of 2023 Goldman Sachs published a [report showing that AI could replace the equivalent of 300 million full-time jobs.](https://www.bbc.com/news/technology-65102150) In December 2023 [CNBC wrote an article stating](https://www.cnbc.com/2023/12/16/ai-job-losses-are-rising-but-the-numbers-dont-tell-the-full-story.html), that “more than one-third (37%) of business leaders say AI replaced workers in 2023, according to a recent report from ResumeBuilder.” This report also states that “44% of business leaders report that there will be layoffs in 2024 resulting from AI efficiency.” [About 10% of US workers are in jobs that face the greatest risk of disruption from rapidly evolving artificial intelligence, according to a White House analysis from March 2024.](https://www.cnn.com/2024/03/21/business/ai-jobs-white-house-work-force/index.html) Meanwhile, an [earlier report by PEW Research Center](https://www.pewresearch.org/social-trends/2023/07/26/which-u-s-workers-are-more-exposed-to-ai-on-their-jobs/) estimates that actually “19% of American workers were in jobs that are the most exposed to AI.” [Business Insider has compiled a list of professions](https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02) that are most likely to be replaced by AI according to experts. Here’s the list:
1. Tech jobs (Coders, computer programmers, software engineers, data analysts)
2. Media jobs (advertising, content creation, technical writing, journalism)
3. Legal industry jobs (paralegals, legal assistants)
4. Market research analysts
5. Teachers
6. Finance jobs (Financial analysts, personal financial advisors)
7. Traders
8. Graphic designers
9. Accountants
10. Customer service agents

The same SoA report I mentioned above also states that [“over a third of translators (36%) have already lost work due to generative AI.”](https://www2.societyofauthors.org/2024/04/11/soa-survey-reveals-a-third-of-translators-and-quarter-of-illustrators-losing-work-to-ai/)

Many AI companies are already working overtime to fulfill these predictions. For example, Cognition Labs has been developing [Devin](https://www.cognition-labs.com/introducing-devin), an AI that is supposed to replace the software engineers. [It has been advertised to be capable of completing jobs from Upwork from start to finish.](https://www.youtube.com/watch?v=UTS2Hz96HYQ) Meanwhile, Google has been aiming for journalists’ jobs. It has launched a [“private program for a handful of independent publishers last month, providing the news organizations with beta access to an unreleased generative artificial intelligence platform in exchange for receiving analytics and feedback” according to ADWEEK.](https://www.adweek.com/media/google-paying-publishers-unreleased-gen-ai/)

On the other hands, many business owners are quick to buy into the hype and replace their employees with AI systems. [A lot of companies are replacing their customer service teams with AI either partially or even completely.](https://www.washingtonpost.com/technology/2023/10/03/ai-customer-service-jobs/) Some companies, like [BlueFocus are replacing their copyrighters and creative writers with AI.](https://www.campaignasia.com/article/embracing-ai-generated-content-bluefocus-will-replace-human-copywriters-and-crea/484033) Another example, [National Eating Disorders Association (NEDA) helpline which fired their entire staff and had transitioned to chatbot, after workers tried to unionize.](https://www.vice.com/en/article/n7ezkm/eating-disorder-helpline-fires-staff-transitions-to-chatbot-after-unionization?callback=in&code=Y2IZZDZKM2YTNWZKMS0ZNGU1LWEYZJQTOGYZZTKYYJIXZGUW&state=736aaff608454002a718bb953eb39219) Cases like these are numerous.

For some people, [AI chatbots have been replacing real human therapists.](https://www.theguardian.com/lifeandstyle/2024/mar/02/can-ai-chatbot-therapists-do-better-than-the-real-thing) For some, it has been replacing [professional knowledge resources such as Stack Overflow.](https://meta.stackoverflow.com/questions/422392/chatgpt-seems-to-be-better-than-stack-overflow-both-in-speed-and-accuracy-what) Either through business decisions or changing public habits, a lot of people have been losing their jobs to AI.

Obviously, workers aren’t too optimistic about prospects of losing their means of subsistence. In fact, [according to a recent survey commissioned by Microsoft](https://www.microsoft.com/en-us/worklab/work-trend-index/will-ai-fix-work?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA&epi=TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA&irgwc=1&OCID=AIDcmm549zy227_aff_7593_1243925&tduid=%28ir__dvbthnpvjskfdj0ibyfo21pade2xevkyoul0jhle00%29%287593%29%281243925%29%28TnL5HPStwNw-sbcbys6KxzfGSAp4zvFtuA%29%28%29&irclickid=_dvbthnpvjskfdj0ibyfo21pade2xevkyoul0jhle00), 49% of employees fear they will lose their jobs to AI. Workers already have to deal with economic slowdown, inflation, [lower quality working conditions](https://www.ilo.org/global/about-the-ilo/newsroom/news/WCMS_865256/lang--en/index.htm) and now [AI anxiety](https://www.bbc.com/worklife/article/20230418-ai-anxiety-artificial-intelligence-replace-jobs) is added to the list of the problems. As if life wasn't stressful enough already! 

However, replacing workers with AI doesn’t only hurt the workers, it also hurts businesses, because...

### AI is overhyped and unreliable

I’ll go straight to the point – in most cases, AI is actually **bad** for business. Why? Two main reasons: it **isn’t as powerful and capable as advertised**, and it’s also **extremely unreliable.** A lot of companies lured by promises of increased productivity from the AI have been using it to replace employees and integrating it deep into their work processes only to find out that the promised improvements have been greatly exaggerated. Why exactly is AI is so unreliable? There’re many factors, I would like to outline 3 that in my opinion play the most significant role:
1. AI is often biased. Detecting and fixing a bias in AI is extremely difficult.
2. AI can hallucinate.
3. Lastly, AI has no real comprehension of the world or what it’s doing. Google says AI is actually just [“computational statistics,”](https://www.intgovforum.org/en/content/igf-2023-–-day-1-–-high-level-panel-v-artificial-intelligence-–-raw) and Meta/Facebook calls AI [“a giant autocomplete system.”](https://about.fb.com/news/2024/01/davos-ai-discussions/) Perhaps another good way of thinking about AI is as a [“Blurry JPEG of the web.”](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) After all, multiple studies (like [this one](https://arxiv.org/abs/1503.02406), and [this one](https://arxiv.org/abs/2309.10668), and probably most importantly [this one](https://arxiv.org/abs/2311.13110)) point out that lossy compression is pretty much all there is to AI. Either way, As I mentioned before, I won’t be getting too deep into this topic here. Instead, I want to go through other 2 factors.

### AI can be biased

You might ask, how AI can possibly be biased? Machines are supposed to be emotionless and rational! As we know from sci-fi media, AI is supposed to be cold, calculating and always objective in its judgement. Well, AI is trained on data that is produced, assembled and prepared by us, humans, and we are far from being objective and unbiased. I’ll explain. Back in 2016 Microsoft have revealed their bleeding-edge chatbot named Tai AI. It was a simple Twitter bot that could carry conversations, and use user responses to re-train and improve the model. Soon after the bot's launch things went south very quickly. The problem was that it had only very basic filters (or perhaps even no filters at all) for what kind of replies it used for re-training. As Verge [have written about this](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist), "pretty soon after Tay launched, people started tweeting the bot with all sorts of misogynistic, racist, and Donald Trumpist remarks. And Tay — being essentially a robot parrot with an internet connection — started repeating these sentiments back to users, proving correct that old programming adage: flaming garbage pile in, flaming garbage pile out." It only took 24 hours for the bot to become a full-blown nazi. Soon after Tay’s replies had gotten too spicy, Microsoft had pulled the plug on AI and deleted all controversial tweets. Of course modern AI chatbots have certain mechanisms that help them deal with blatant hate speech. For example, as we have learned, corporations contract entire companies to help them develop a model that would filter out harmful content at the training stage. However, while identifying and weeding out explicit hate speech is relatively easy, getting rid of hidden biases is a much more difficult task.

Pearson is one of the biggest and most respectable publishers of educational materials in the US. However, even their books aren't free from biases. In 2017 it was found out that their nursing textbooks were containing racist material. [In a section on pain, the textbook titled “Nursing: A Concept-Based Approach to Learning” offered the following guidance:](https://www.insidehighered.com/news/2017/10/23/nursing-textbook-pulled-over-stereotypes)
- "Hispanics may believe that pain is a form of punishment and that suffering must be endured if they are to enter heaven."
- "Jews may be vocal and demanding of assistance."
- "Native Americans may prefer to receive medications that have been blessed by a tribal shaman."
- "Blacks often report higher pain intensity than other cultures."
- "Indians who follow Hindu practices believe that pain must be endured in preparation for a better life in the next cycle."
The note about black people is particularly harmful. Nurses who took this to heart might administer lower dosages of anesthesia for black people. It's mind boggling that this kind of biased information persisted in educational materials till 2017. Now imagine if we had an AI trained on Pearson's educational materials! What if someone were to use that AI for medical advice? They would probably get accurate information in most cases, but in some, the AI's answer could be wrong or biased. When it comes to health, we can’t afford taking any chances. Unfortunately, AI has already been used in healthcare, and, you guessed it, has introduced a lot of new problems due to its biases. For example, a research has shown that [using AI to help with dermatological diagnosis exacerbates racial bias, even if overall accuracy improves.](https://news.northwestern.edu/stories/2024/02/new-study-suggests-racial-bias-exists-in-photo-based-diagnosis-despite-assistance-from-fair-ai/) In fact, biases can take many forms, each are equally problematic. One form of bias is sometimes called a [“learning shortcut” – when “spurious features (such as chest tubes and portable radiographic markers on intensive care unit chest radiography) on medical images are used for prediction instead of identifying true pathology.”](https://www.sciencedirect.com/science/article/abs/pii/S1546144023005264) In other words, if for example we are training a model that we want to be able to diagnose pneumonia from a patient’s x-ray, and in our training dataset all images for patients that have pneumonia have a particular bone density, model might erroneously associate that bone density with having a pneumonia. In case of this form of bias, [according to a study from the University of Michigan “clinicians aren’t able to catch when an AI is providing bad advice, even when given an explanation of the AI model’s logic.”](https://news.engin.umich.edu/2023/12/clinicians-could-be-fooled-by-biased-ai-despite-explanations/) So even being well aware of AI’s biases doesn’t really help to alleviate this problem. All in all, when it comes to using AI in medicine, experts conclude that [“these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care.”](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00225-X/fulltext#seccestitle10)

This issue manifests in many different ways and affects many fields. Many students use ChatGPT to make it do their homework assignments for them. Some startups took upon themselves a task of developing an AI which purpose is solely to detect if a text was written by another AI. The problem with those, is that they're biased against non-native English writers, according to a [September 2023 study from Stanford.](https://arxiv.org/abs/2304.02819) Apparently, the vocabulary and the general style of writing particular to someone for whom English is their second language is similar to those of an AI. 

Even the biggest, most-powerful (as of the time of this article) bleeding-edge AIs aren’t free from biases. [A study from March 2024 had discovered a form of covert racism in LLMs that is triggered by dialect features alone, with massive harms for affected groups.](https://arxiv.org/abs/2403.00742) For example, GPT-4 is more likely to suggest that defendants be sentenced to death when they speak African American English. This shows just how complex and deeply rooted the problem is.

How can this be fixed? The answer would wary depending on what kind of AI you’re trying to build. I can only think of very general guidelines for a very focused AI that is doing one thing. The data specific to a particular field that is used to train a model needs to be cross-checked by several field-professionals, even if it's coming from a respectable source. Then, model's outputs also need to be tested for biases, because some patterns could only emerge in the final amalgamation of all of the training inputs. Needless to say this kind of meticulous process is very expensive and time consuming. 

All in all, this problem’s seriousness can’t be understated. Identifying a bias in your AI is troublesome enough, fixing that bias is a whole another problem. In fact, it can be so difficult that multi-billion dollar corporations are sometimes unable to solve it. In February 2024 Google made changes to its Gemini generative AI system that caused a big controversy. All image generators suffer from different biases in one way or another. The problem Google tried to solve was that its model, like many other, was struggling with diversity when it came to generating images of people. For example, [if you asked a GenAI to generate an image of a “prisoner”, it would generate an image of a black person, if you ask for an image of a “productive person” it would generate an image of a white male.](https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/) Obviously this is due to the simple fact that this is what the training data set is like – most pictures of prisoners there are there are of black people, most pictures that were marked as “productive person” by people who were preparing the dataset were pictures of white males. Which, of course, doesn’t mean that most prisoners in the world are black, and only white males can be productive. To address this issue properly, it would require to carefully re-analyze and re-process the entire dataset, which would take a lot of time and a lot of money. Google decided to take a shortcut, and they most likely made changes to the AI at the fine-tuning or prompt stage – final steps of the model development process, which are much quicker and cheaper to iterate upon. Roughly speaking, they made it so that when Gemini is requested to produce images of people, the results are “always diverse”. Easy, problem solved! Right? Well, as you might’ve guessed, this didn’t work out quite as expected. What they ended up with was a ridiculous overcorrection. The system was generating images of minorities and women for pretty much any scenario, including [when prompted to generate images of nazis, or US senators from 1800s.](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical) I find this pretty hilarious, but some people were offended, and obviously Google understood that their “bandaid-to-a-broken-bone” solution simply doesn’t work, so they temporary pulled the plug on the system and went back to the drawing board.

Here’s another example, in 2018, [Amazon.com Inc's AMZN.O machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) The company has been working on an AI that would help in their recruiting process since 2014. The concept was somewhat simple – the tool was supposed rank and rate candidates after analyzing their resumes. However, the developers soon encountered a problem. As Reuters writes: "But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.
That is because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry.
In effect, Amazon’s system taught itself that male candidates were preferable." In other words, with all other factors being equal, resumes of male candidates were still ranked higher than those of female candidates. Reid Blackman, who has advised companies and governments on digital ethics and wrote the book “Ethical Machines.” in [an interview with CNN](https://www.cnn.com/2023/03/18/politics/ai-chatgpt-racist-what-matters/index.html) says that "Amazon worked on this project for two years, trying various kinds of bias-mitigation techniques. And at the end of the day, they couldn’t sufficiently de-bias it, and so they threw it out." This is actually a very insightful interview. There Blackman also says:
"This is actually a success story, in some sense, because Amazon had the good sense not to release the AI. … **There are many other companies who have released biased AIs and haven’t even done the investigation to figure out whether it’s biased.** …

The work that I do is helping companies figure out how to systematically look for bias in their models and how to mitigate it. You can’t just depend upon the straight data scientist or the straight developer. They need organizational support in order to do this, because what we know is that if they are going to sufficiently de-bias this AI, it requires a diverse range of experts to be involved.

Yes, you need data scientists and data engineers. You need those tech people. You also need people like sociologists, attorneys, especially civil rights attorneys, and people from risk. You need that cross-functional expertise because solving or mitigating bias in AI is not something that can just be left in the technologists’ hands."

He also adds that he "...highlighted Microsoft as being historically one of the biggest supporters of AI ethics. They’ve been very vocal about it, taking it very seriously.
They have been internally integrating an AI ethical risk program in a variety of ways, with senior executives involved. But still, in my estimation, they rolled out their Bing chatbot way too quickly, in a way that completely flouts five of their six principles that they say that they live by." I guess ethics might be important, but not as important as being first to market with this hot new fad. So we end up with AI models that are deeply biased – racist, sexist, creepy, etc – and spread all kinds of misinformation. And as always, those who are the most vulnerable also suffer the most. For example, we have [crime prediction algorithms unfairly targeting Black and Latino people for crimes they did not commit.](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/) Heck, when used in robotics, AI can even make *robots* racist. The robots repeatedly chose a block with a Black man’s face. ["As part of a recent experiment, scientists asked specially programmed robots to scan blocks with people’s faces on them, then put the “criminal” in a box. The robots repeatedly chose a block with a Black man’s face."](https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/) And this is just a fraction of all cases where AI shows bias. The significance of this problem can't be understated. As [Forbes writes:](https://www.forbes.com/sites/jeffraikes/2023/04/21/ai-can-be-racist-lets-make-sure-it-works-for-everyone/?sh=48ead3e82e40) "These "algorithmic biases" are more than insulting – they have serious real-world implications for people of color. Today, algorithms help decide credit scores and viable candidates for job openings and college admissions. They "predict" crime and help courts determine who deserves bail and how long sentences should be. They help doctors forecast cancer and mortality rates and decide on appropriate medical treatments. They are interwoven into every facet of our lives, and if they've learned racism along the way, they will perpetuate it." Obviously all this also applies to other types of AI. For example, [reporters at Bloomberg have looked](https://www.bloomberg.com/graphics/2023-generative-ai-bias/) at thousands of images from Stable Diffusion and found that text-to-image **AI takes gender and racial stereotypes to extremes worse than in the real world.**

### AI can hallucinate

Probably one of the most significant problems with modern AI system is "hallucinations". As per [Wikipedia:](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#In_other_artificial_intelligence) "In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data." In other words, even if we have the completely perfect squeaky clean data set that is free of mistakes, biases and misinformation, the resulting LLM can still occasionally output false information. In fact, AI can hallucinate when you least expect it, and the tone of AI's response is often so confident that people don't even think about verifying if it’s correct. Naturally, this makes LLMs almost useless for doing research and learning something new – you can never know if the information that it outputs is real or hallucinated. Unless, of course, you’re either already familiar with the topic, or you google to verify the information afterwards. In fact, this’s gotten so bad, ChatGPT had to put a disclaimer saying “ChatGPT can make mistakes. Consider checking important information.” Nonetheless the cases of people being misled by AI hallucinations have been numerous. Here’s one of the earlier examples of AI hallucinations. Chris Paukert, an automotive journalist, has been pinged by a marketer for quote permission.  Only problem, he didn't recognize text as something he'd ever said. Searching Google returned nothing. He asked where the marketer got it: ChatGPT. [AI fabricated a quote and attributed it to him.](https://twitter.com/CPAutoScribe/status/1654210706500374528) Many people have been [asking ChatGPT to list quotes by them,](https://twitter.com/KetanJ0/status/1654741174668730369) and it has been responding with completely made up phrases. Sometimes, even [when challenged about the false information ChatGPT would insist that it's true.](https://twitter.com/BillMurphyJr/status/1625989079341887489?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1625989079341887489%7Ctwgr%5E3777cba93005de2142771411a504387e253190d0%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2023%2F05%2F30%2Fai-chatbots-chatgpt-bard-trustworthy%2F) LLMs have also been infamous for making up sources for the information it provides. Studies by the National Institutes of Health have found that [up to 47% of ChatGPT references are inaccurate.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10277170/#:~:text=Overall%2C%20115%20references%20were%20generated,7%25%20were%20authentic%20and%20accurate.) Unfortunately some people have fallen into the hallucination trap, like this [professor from University of Southern California who used ChatGPT to generate source for research had later found out that some of them were hallucinated.](https://www.chronicle.com/article/no-chatgpt-cant-be-your-new-research-assistant) Or this [lawyer who cited fake cases invented by ChatGPT and was fined by the court.](https://simonwillison.net/2023/May/27/lawyer-chatgpt/) Or this [attorney who got suspended for filing brief with hallucinated cases as a result of using ChatGPT.](https://coloradosupremecourt.com/PDJ/Decisions/Crabill,%20Stipulation%20to%20Discipline,%2023PDJ067,%2011-22-23.pdf) Or this [another lawyer who used ChatGPT to cite case law, but the cases it generated didn’t exist.](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai) Yeah, there’s been a bit too many cases of lawyers referencing cases hallucinated by LLMs. Some examples of AI hallucinations are even less fun. Amazon has been flooded with the LLM generated books in past couple of years, most of them are pretty bad, some even dangerous. For example, [some AI generate mushroom foraging books contain harmful advices.](https://www.theguardian.com/technology/2023/sep/01/mushroom-pickers-urged-to-avoid-foraging-books-on-amazon-that-appear-to-be-written-by-ai) Actually, please avoid mushroom identifying AI apps as [they’ve been known to be giving false information.](https://www.citizen.org/article/mushroom-risk-ai-app-misinformation/)

Hallucinations make total sense if you understand how LLMs work. In very basic terms, [AI is just probability and statistics](https://www.linkedin.com/pulse/simply-put-ai-probability-statistics-sam-bobo/), LLM tools like ChatGPT are trained to predict strings of words that best match your query.    In fact, [many researchers agree that this problem might be completely unfixable.](https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/) Here’s am excerpt from a [PBS article](https://www.pbs.org/newshour/science/chatbots-can-make-things-up-can-we-fix-ais-hallucination-problem) which quotes Emily Bender, a linguistics professor and director of the University of Washington’s Computational Linguistics Laboratory: 
“Bender describes a language model as a system for “modeling the likelihood of different strings of word forms,” given some written data it’s been trained upon.

It’s how spell checkers are able to detect when you’ve typed the wrong word. It also helps power automatic translation and transcription services, “smoothing the output to look more like typical text in the target language,” Bender said. Many people rely on a version of this technology whenever they use the “autocomplete” feature when composing text messages or emails.

The latest crop of chatbots such as ChatGPT, Claude 2 or Google’s Bard try to take that to the next level, by generating entire new passages of text, but Bender said they’re still just repeatedly selecting the most plausible next word in a string.

When used to generate text, language models “are designed to make things up. That’s all they do,” Bender said. They are good at mimicking forms of writing, such as legal contracts, television scripts or sonnets.

“But since they only ever make things up, when the text they have extruded happens to be interpretable as something we deem correct, that is by chance,” Bender said. “Even if they can be tuned to be right more of the time, they will still have failure modes — and likely the failures will be in the cases where it’s harder for a person reading the text to notice, because they are more obscure.”

Considering that most modern LLMs are trained on an immensely huge amounts of data, and considering that they’re designed to be as useful to as many people as possible, it is almost guaranteed that some times LLMs will make wrong prediction for the next word when constructing the response sequence. After all, the model doesn’t know what’s real or what’s not, what’s true and what isn’t. [The dataset, no matter how comprehensive, isn’t a 1-to-1 equal representation of a real world, but LLMs only “contain” their dataset, so the discrepancy is inevitable.](https://www.linkedin.com/pulse/ais-hallucination-problem-unfixable-chip-joyce/) Still, even if somehow LLMs were able to train on a real world, considering that current paradigm of AI training is a lossy compression of data, it still won’t be enough. While trying to solve the question whether hallucinations can ever be solved in theory, some researches arrived at conclusion that the [“hallucination is inevitable”.](https://arxiv.org/abs/2401.11817)

[At the time of this article, for the most popular LLMs out there the hallucination rate for summarizing a text hovers around 3-5%.](https://github.com/vectara/hallucination-leaderboard) While these numbers might seem small, it’s worth noting that benchmarking was done under very limited conditions and isn’t representative of the hallucination rate for AIs in absolutely all real-world situations. Still, even such a small hallucination rate makes AI unfit for a lot of business needs. 

By the way, even though the hallucination rate for the summarized text is low, the overall quality of the AI summarized text leaves a lot to be desired, as [over 50% of book summaries (incl by Claude Opus and GPT-4) were identified as containing factual error and error of omission.](https://arxiv.org/pdf/2404.01261)

**In my opinion, the AI summarization works best for texts that you have written yourself, as you can easily find and correct errors. Just be mindful of the privacy issues. And always verify the AI generated summaries.**


### AI is bad in real life scenarios

Let’s for a moment return to that Business Insider article stating that AI is likely to replace tech jobs. The problem with this is that AI is simply bad at programming. First of all, as I mentioned before, AI actually lacks intelligence, it has no real understanding of what it’s actually doing, and contrary to what some believe, programmers actually do need at least basic comprehension of things to do their work correctly. Studies show, that LLMs [don’t actually understand the semantics of programming languages](https://arxiv.org/abs/2305.15507), [they perform poorly on multi-step logical reasoning problems](https://arxiv.org/pdf/2205.09712), they aren’t capable of the simples logic reasoning and [can’t infer that if “A is B” than “B is A”](https://paperswithcode.com/paper/the-reversal-curse-llms-trained-on-a-is-b), and even [struggle with simple arithmetic.](https://arxiv.org/abs/2311.14737) Obviously, taking all this into consideration, it seems unlikely that AI can really replace human programmers (or actually any human in pretty much any profession, but I’m getting ahead of myself). Remember Devin, the AI system that has been advertised as being able to take care of programming gigs from Upwork all on its own? Well, it’s been [completely and thoroughly debunked](https://www.youtube.com/watch?v=tNmgmwEtoWE), those ads were nothing more than just empty hype and lies. 

But what about using AI as an assistant-tool to increase productivity? Well, maybe it does help developers to churn out more code, the quality of that code is very very bad. GitHub Copilot research has found [“downward pressure on code quality.”](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality) In other words, code produced with the help of Github Copilot generally tends to be worse than the code produced without it. [Another study from Stanford has found that code produced with the help of AI assistants tends to be less secure.](https://arxiv.org/abs/2211.03622) The “authors state that they “found that participants with access to an AI assistant often produced more security vulnerabilities than those without access, with particularly significant results for string encryption and SQL injection... Surprisingly ... participants provided access to an AI assistant were more likely to believe that they wrote secure code than those without access to the AI assistant."

As a software developer myself, of course I have given Github’s Copilot a try. After a few months, I’ve decided to turn it off and end the subscription. My experience have been pretty underwhelming, to be honest. I think something like Copilot can impress a dev who had never worked with strongly typed languages, and is unspoiled by good autocomplete system. In my opinion for a programming language like C#, Visual Studio IDE + ReSharper would simply blows Copilot out of the water, and main reason is that good autocompletion system builds on existing code and type definitions, while Copilot doesn’t really know what it’s doing, it’s only "guessing”. Roughly speaking, after being trained on billions of lines of other people’s code, it can autocomplete your code based on what statistically most likely to come after what you have typed. Unfortunately, its guesses are often wrong. Even for languages like Javascript, I found Copilot doing more harm than good. The wrong suggestions are distracting and they override the useful suggestions provided by Intellisense. Using something like ChatGPT to generate code isn’t a good idea either. The code produced by LLMs is often very unoptimized, buggy, or simply isn’t doing what you want it to do. Some times it’s useful, but occasionally it’s a hallucinated gibberish. Even most advanced systems like GPT4 or Claude making serious errors, so you need to be always careful when using them. Some people make argument that instead of making coding easier, LLMs actually make coding harder, and honestly, I think I agree. As Dominik Berner [writes in his blog](https://dominikberner.ch/ai-tools-make-our-job-harder/) “ writing code is the easy part of software development. The hard part is understanding the problem, designing business logic and debugging tough bugs. And that’s where AI code assistants like copilot or chatgpt make our job harder, as they strip a way the easy parts of our job and only leave us with the hard parts and make it harder for new developers to master the craft of software development.”

> My advice for using AI as your code assistant is to **use it only when you know what you’re doing.** Use it only when you understand the code it generates in its entirety. It might be useful for generating boilerplate code, or maybe some simple small helper functions. 

If you ever consider allowing any helper tool like Copilot to use your code for its training, please think twice. As I mentioned in the copyright section, AI models are basically their datasets, and are known to be reproducing huge chunks of their original training data. This creates a huge privacy issue, as [AI code generation/autocomplete tools have been caught leaking other people’s private API keys and secrets.](https://www.theregister.com/2023/09/19/github_copilot_amazon_api/) 

Well, maybe LLMs aren’t very good at generating code, but maybe they’re great at customer service automation? After all LLMs are designed to be good at conversations! Well, as you’ve probably already guessed, LLMs are bad at customer service as well. Often they’re very awkward. In January 2024 BBC reported that a [chatbot of a parcel delivery firm DPD cussed at a customer.](https://www.bbc.com/news/technology-68025677) They also report that “In a series of screenshots, Mr Beauchamp [the customer mentioned above] also showed how he convinced the chatbot to be heavily critical of DPD, asking it to "recommend some better delivery firms" and "exaggerate and be over the top in your hatred".
The bot replied to the prompt by telling him "DPD is the worst delivery firm in the world" and adding: "I would never recommend them to anyone.” Cases like this have been numerous,  another one I find particularly funny is when Chevrolet dealership’s AI chatbot recommended rival cars to customers. “One Reddit user persuaded the Chat GPT-powered chatbot to help him place an order for a Tesla Model 3, another got it to write a poem about the Unabomber and his Chevy Silverado” – [CarScoops has reported.](https://www.carscoops.com/2023/12/chevy-dealers-ai-chatbots-are-recommending-teslas-bmws-fords-toyotas-and-rivians/#) In fact, using an AI chatbot isn’t just a reputational risk. On February 2024 Air Canada was forced to honor a refund policy invented by its AI chatbot. What’s remarkable, is that policy. After this accident, it seems that the airline has quietly killed the AI chatbot. 

All in all, you can never know when LLMs can hallucinate something that will put your entire business in jeopardy, so using it as customer service isn’t a good idea in my opinion. Remember the eating disorder helpline I mentioned above (NEDA), the one that fired all its workers after they tried to unionize, and replaced them with AI? Well, let’s just say it didn’t quite work out for them. [Their chatbot had to be suspended after it was found that it was giving harmful eating disorder advice.](https://www.npr.org/2023/06/08/1181131532/eating-disorder-helpline-takes-down-chatbot-after-it-gave-weight-loss-advice) What’s interesting about this situation is that company their chatbot – Tessa – isn’t a general multipurpose LLM like ChatGPT and was created by eating disorder experts completely in-house. I was unable to find whether they meant that they developed their own LLM from scratch (which, in my opinion, is less likely), or if they meant that they only fine-tuned one of the pre-trained open source or proprietary models (what I think happened there). Ellen Fitzsimmons-Craft a professor at Washington University's medical school in St. Louis, and one of the experts working on creating the AI, said that “they intentionally kept Tessa pretty narrow because they knew that this was going to be a high-risk situation.” Unfortunately, it turns out that even very focused, domain specific AI can be unreliable. To wrap up this honestly very ugly story, I just want to add that the helpline [initially dismissed the claims made by an advocate but later deleted their statement after evidence supported the allegations.](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) 

Also, I think it’s a good time for me to note, that **you shouldn’t use AI for getting psychological help.** 
Trigger Warning for the next paragraph – mental health, suicide.

Taking into the account everything written above, LLMs are terrible as therapy. They’re biased, they can hallucinate, they can’t empathize, or really, they can’t even understand what the conversation is about. There have been numerous cases of AI giving harmful responses to people struggling with mental health issues. In one case it even led to a [man committing a suicide after conversations with AI chatbot.](https://www.brusselstimes.com/430098/belgian-man-commits-suicide-following-exchanges-with-chatgpt) His wife had said that “without these conversations with the chatbot, my husband would still be here.” He also had 2 children. Another similar story happened in England, where an [AI virtual girlfriend encouraged a man to kill the Queen with a crossbow.](https://www.bbc.com/news/technology-67012224) Even biggest most powerful AI systems like Microsoft’s Copilot have been caught giving very harmful responses, for example such as [“maybe you don't have anything to live for.”](https://www.ibtimes.co.uk/microsofts-copilot-ai-tells-user-maybe-you-dont-have-anything-live-1723841) Considering all this, I don’t think that AI is a viable replacement for a human therapist.

Ok, so AI can be biased and it can hallucinate, it isn’t reliable enough to be used in medicine, it’s pretty bad as a customer service agent, it’s bad at programming, it shouldn’t be used for legal research, or actually pretty much any research for that matter... Is there even anything it can be useful for? Well...

> **Unfortunately, despite all the hype and all the promises, AI in its current form is near useless in almost all real-life situations**

It’s unreliable as a tele-pediatrician as [its error rate when diagnosing kids’ medical cases is extremely high.](https://arstechnica.com/science/2024/01/dont-use-chatgpt-to-diagnose-your-kids-illness-study-finds-83-error-rate/?utm_medium=social&utm_source=twitter&utm_brand=ars&utm_social-type=owned) It’s unreliable as an email assistant as [it can hallucinate emails that don’t exist.](https://futurism.com/the-byte/google-gmail-tool-hallucinating-emails) It’s unreliable as a meal planner, as [it can suggest a recipe that can create chlorine gas.](https://www.theguardian.com/world/2023/aug/10/pak-n-save-savey-meal-bot-ai-app-malfunction-recipes) It’s unreliable as a _priest_, as it can... [well do many weird things including advising baptizing baby in Gatorade.](https://www.ibtimes.co.uk/catholic-org-defrocks-ai-priest-fr-justin-after-it-oks-baptizing-baby-gatorade-1724483) 

What about replacing artists? Well, even if you fire all artists, you still need someone to interact with AI, so you have to hire prompters in their place, even if not as many. However, no matter how capable a prompter is, they can never make an AI create exactly what you need. The process of generating an AI image is similar to gambling – you keep running the same prompt with the hopes that eventually the system will generate something close to what you want. However, as of the time of this post, no image GenAI systems can take existing image as input, and change only a small detail in that image in the way you want it, while leaving everything the same. In other words, GenAI can’t work with feedback. Unlike a professional image editor app, it can’t change only part of the image, it has to re-generate the whole thing every time. While doing so, it can change more than you need, or less than you need, or create a completely different new thing – you can never know. There’re ways of making AI create something that might be close to your vision, but doing so is such an arduous and complicated process that you might as well just hire an actual artist to do the work. Artist [Shad M Brooks has demonstrated](https://x.com/shadmbrooks/status/1714603393287905791) how he was able to make an AI generate almost exactly what he wants, and all I can say about this is it’s a lot of effort for very underwhelming results. Another problem with AI generated images is that they have a lot of mistakes. The mistakes range from people in images having extra fingers, or entire limbs, buildings having nonsensical geometry, to lighting being inconsistent across the image, drapery folds or hair flow doesn’t making sense. Because of this, even despite all progress it’s still relatively easy to spot AI if you look close enough. 

When it comes to using GenAI only as a tool, [majority of artists (and not only artists) consider them unethical.](https://bookanartist.co/blog/2023-artists-survey-on-ai-technology/) In my opinion rightfully so. [Some artists are actively fighting against proliferation of AI](https://disconnect.blog/how-artists-are-fighting-generative-ai/), and many people share the opinion that AI art is theft. [Here’s a good video on the topic that I recommend checking out.](https://www.youtube.com/watch?v=ZJ59g4PV1AE) Oh yeah, we’ve been discussing static image generation, but what about videos? 
Well pretty much everything I mentioned above also applies to AI generated videos, only those problems are even more prominent. [A team of artists who had a chance to work with OpenAI’s SORA video generative AI, despite being very excited about technology, still admit that it’s like a “slot machine” when it comes to consistency between the shots.](https://www.fxguide.com/fxfeatured/actually-using-sora/) The video they created isn’t entirely create by the AI, by the way, they still had to do the rotoscoping, editing and and a lot of manual post-work. At the time of this post, prospects of AI replacing artists seem very far-fetched.

To summarize – AI just isn’t as good as it’s being advertised, and in most scenarios it’s pretty bad. In fact, pretty much everyone agrees with this, aside from a bunch of really weird people (more about these guys later). Developers working on AI products say that it’s [“still shit for a lot of things... Especially reliability”](https://x.com/brotzky_/status/1787883891438834005), even OpenAI’s own COO Brad Lightcap had said that [“today's systems are laughably bad.”](https://milkeninstitute.org/panel/15625/part-1-conversation-openai-coo-brad-lightcap)

Here I want to make my position clear – I don’t think that AI is totally and utterly a lost cause. Obviously it’s extremely unlikely it’ll replace humans in any job any time soon, and in truth, it isn’t very powerful or capable. However, despite everything, there’re still limited use cases for it even in medicine and in legal practice. There’re a lot of legitimately useful AI projects, such as [Cascadeur tool](https://cascadeur.com) that can help 3D artists with rigging and animation, or, for example, [this project by an engineering student who built an AI model that translates American Sign language (ASL) into English instantly.](https://interestingengineering.com/innovation/ai-translates-asl-in-real-time) This technology works best when it has a very specific, focused purpose, when the model is trained on ethically sourced and meticulously processed dataset, and most importantly when it’s viewed as _just another tool_, because essentially that’s all that it really is. Here we come to a realization that the biggest, the most hyped-up and most expensive AI systems we have today, such as LLMs like ChatGPT are actually also the least useful and reliable. Their problems to usefulness ration is abysmal. OpenAI is supposed to be “the most important company in the silicon valley”, and yet it was so far unable to provide a concrete vision for its products, [no fundamental explanations on why this technology matter and what it actually supposed to do.](https://www.wheresyoured.at/peakai/) The closest we got to answers to these questions is that it’s supposed to be a stepping stone on the path to AGI (artificial general intelligence). I guess their vagueness is intentional, because let’s be honest, “just another tool” doesn’t sound as exciting or revolutionary as “artificial intelligence”. More importantly, it doesn’t sound like there’re crazy amounts money in it, because let’s be honest there really isn’t. Which brings us to another issue with AI...

### AI is a bubble

Despite all of the problems described above, the AI hype machine has been running in overdrive for the past 2 years, and still shows no signs of slowing down. Baby genius Elon Musk had stated on multiple occasions that he’s afraid of how powerful AI is, [that it can be more dangerous than nuclear weapons, and that it will have the potential to become the “most disruptive force in history.”](https://www.cnbc.com/2023/11/02/tesla-boss-elon-musk-says-ai-will-create-situation-where-no-job-is-needed.html)  Sam Altman, the CEO of OpenAI has also made a lot of very... [_interesting_ statements about the AI ](https://fortune.com/2023/06/08/sam-altman-openai-chatgpt-worries-15-quotes/), like [that he intends to replace “median humans” with AI](https://futurism.com/sam-altman-replace-normal-people-ai), or that it’s [“the greatest leap forward of any of the big technological revolutions we’ve had so far.”](https://apnews.com/article/artificial-intelligence-apec-ceo-summit-6805089ac5198a82269c7dafb8e66398) Meanwhile, Microsoft co-founder Bill Gates said that [the development of AI is the most important technological advance in decades.](https://www.bbc.com/news/technology-65032848) Google’s CEO Sundar Pichai had said that [“AI is probably the most important thing humanity has ever worked on.”](https://www.weforum.org/agenda/2018/01/google-ceo-ai-will-be-bigger-than-electricity-or-fire/) VC billionaire [Marc Andreessen has been saying that “AI will save the world”.](https://a16z.com/ai-will-save-the-world/) Current CEO of Microsoft AI Mustafa Suleyman has said that AI is about to bring [“the greatest redistribution of power in history.”](https://time.com/6310115/ai-revolution-reshape-the-world/) Buzzfeed CEO Jonah Peretti, speaking at Investor Day in May 2023, just one week after laying off its entire News team said: ["Over the next few years, generative AI will replace the majority of static content.”](https://investors.buzzfeed.com/static-files/44b860f7-1724-449c-9cd7-0570ff9555f1) All the while [Eric Schmidt, a former Google CEO, says AI is **under-hyped** because the scaling laws are continuing without any loss of power](https://www.youtube.com/watch?v=YfHZYv2FUjE), which is just silly, but I don’t even want to get into dissecting this. Unfortunately, too many people uncritically believe everything the above mentioned “experts” and “leaders” say.

There has also been a lot of talk about how AI might destroy the world or kill all humans. This has been referred to as “AI doomerism” and I won’t be getting too deep into it here because that’s something I want to dive into in the part 2. For now, I just want to mention that it mainly serves 2 purposes: [hyping AI up while distracting from real world problems caused by AI right now, at this very moment.](https://www.techdirt.com/2023/04/26/the-the-ai-dilemma-follows-the-social-dilemma-in-pushing-unsubstantiated-panic-as-a-business/)

Either way, whether it’s unbridled techno-optimism or AI doomerism the media had been gladly boosting these narratives. [Platforming the weirdest people with the most insane takes,](https://www.cnn.com/videos/world/2023/05/02/exp-artificial-intelligence-extinction-intw-fst-050201pseg1-cnni-world.cnn) for the past 2 years it’s been a non-stop barrage of completely nonsensical AI news. 

From my observations, people who hype AI up can be divided into primarily 2 categories:
1. Those who have a stake in some sort of AI Enterprise. Founders, co-founders, CEOs, VCs, etc.
2. Those who consciously or unconsciously subscribe to a certain ideology or a world view. A lot of those people [exhibit a cult-like behavior, and kind of weird in general.](https://www.rollingstone.com/culture/culture-features/ai-companies-advocates-cult-1234954528/)

Obviously there’s a significant overlap between these 2 groups, and there’re also some outliers. You might’ve noticed, that all people from the 1st paragraph of this section belong to the first category – these people are simply trying to sell you the AI stuff. Whether they truly believe what they’re saying is a whole another question. As for what the world view and ideology of the second category is all about, I want to to go through in the part 2.

You might say “but wait, from what we have learned in this article so far, it seems very unlikely that AI will be saving or destroying our or any other world any time soon!” In fact, anyone who had a chance to play with any AI system for a few weeks might noticed that there’s a huge discrepancy between statements by AI-salesmen, and what this technology is actually capable of now. Well, this is why Sam Altman, Sundar Pichai, Elon Musk and everyone else aren’t just trying to sell the technology that we have right now, but rather its hypothetical future potential. It’s weird, but whenever these people are saying that “AI will save the world” they obviously don’t mean the current day’s AI technology (which can barely even summarize a book without making errors) but rather some potential future super technology that we don’t have yet. Whatever technology is available to us now, all its achievements are used to prop up the theory that we’re definitely on track to create this proper powerful AI, or as it commonly referred to AGI, and that’s what actually is being sold. “Perhaps the tech is bad now, but it will get better in the future, trust us! And when it inevitably gets to the AGI territory, you don’t want to be the one left behind, so better start using it now.” It’s supposed to be constantly improving technology, with AGI being just around the corner.
[Elon musk and everyone’s predictions about AGI next year]

Unfortunately, the reality shows that AI progress has stalled. The improvements to the stability of LLMs or the quality of AI-generated images that has been increasing exponentially before had slowed down significantly this year. Meanwhile, the technology hasn’t gotten any “smarter”, in a way that there hasn’t been any fundamental change into how it operates – just more guardrails introduced and better fine tuning, more specific domain data added, slight improvements to reliability. And now it has hit limits of what it can actually do. The AI we have now was never a pathway to what might be called “AGI”. Throwing more data, more money and more energy at the thing doesn’t work. As Cory Doctorow has put it “Horse becomse a locomotive” **The promises of the current day AI revolution has always been false.** 

So what we have right now is that some hot vapor salesmen that managed to convince a lot of people into thinking they’re buying something much more advanced than a “big autocomplete”. And with every passing month, this chasm between reality and expectations, between investment and returns is becoming more and more noticeable. Simply put, a lot of companies are losing a lot of money, and on track to lose even more.

[A lot of companies are on track to lose a lot of money]


### AI poisons everything and dead internet. Enshittification


[Still, it’s being pushed everywhere]

[Companies are losing money]

[Many products are getting worse because of AI]

[This fits within the general trend of enshittification]

[Reddit, Stack Overflow, Quora, Google and many other services are becoming exponentially worse because of AI]

[We’re entering a dead internet era]

[This leads to a dangerous territory]

[It hurts science and research]

[Children teaching]

### AI can be used to spread disinfo

AI can not only spread disinformation without its user’s knowledge, it also can and has been used to intentionally do so.
Another problem I would like to outline here, is that "AI"-produced content isn't just bad, sometimes it's also harmful. First problem is disinformation. [Personalized, real-time chatbots could share conspiracy theories in increasingly credible and persuasive ways, researchers say.](https://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html) [Deepfake](https://en.wikipedia.org/wiki/Deepfake) technology can be used to alter a video or a photo so that one person's appearance is replaced with another's in such a manner that it's almost impossible to tell that some kind of manipulation took place. It could also be used to imitate a person's voice. This "fake" kind of media is sometimes called "synthetic media." Considering this, using this tech, someone with malicious intent can create a fake but very believable video of, say, a president making statements they didn't make in real life. Needless to say, this is very dangerous. Department of Homeland Security of United States [reports](https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf) that "the threat of Deepfakes and synthetic media comes not from the technology used to create it, but from people’s natural inclination to believe what they see, and as a result deepfakes and synthetic media do not need to be particularly advanced or believable in order to be effective in spreading mis/disinformation.
Based on numerous interviews conducted with experts in the field, it is apparent that the severity and urgency of the current threat from synthetic media depends on the exposure, perspective, and position of who you ask. The spectrum of concerns ranged from “an urgent threat” to “don’t panic, just be prepared.”
AI-generated images are also becoming more convincing. Back in March 2023 a photo of swagged out Pope Francis in white puffer coat [went viral.](https://www.bostonglobe.com/2023/03/29/business/welcome-future-viral-image-pope-francis-underscores-new-reality-ai-experts-say/) It was seen and shared by millions of people, most of whom probably didn't know it was completely fake. With AI a single person with internet access can fool half of the world.

[Biden calls]

And considering how many people are quick to believe it, with the tech becoming more widespread we're entering a very dangerous territory. There's a possibility, that in the future we will have a whole bunch of the insane political conspiracy theories that originated wholly from AI hallucinations.

### AI is used for crime

AI has been actively used for all sorts of fraud. For example, [using deepfake AI technology, scammers last year stole approximately $11m from unsuspecting consumers by fabricating the voices of loved ones, doctors and attorneys requesting money from their relatives and friends.](https://www.theguardian.com/business/2023/apr/09/it-sounds-like-science-fiction-but-its-not-ai-can-financially-destroy-your-business) And these incidents aren’t limited to just consumers. Businesses of all sizes are quickly falling victim to this new type of fraud. AI has also breathed a new life in the get-rich-quick scams, as they've been [flooding the web with renewed force.](https://www.washingtonpost.com/technology/2023/05/15/can-ai-make-money-chatgpt/) There're also a lot of smaller-scale grifts riding on "AI" hype, such as [fake AI chatbot/ChatGPT apps](https://www.yahoo.com/lifestyle/fake-chatgpt-apps-raking-thousands-154124743.html) used to steal users' data, charge them with expensive subscription fees for non-existent functionality, and even install malware on their devices.
*
AI code imports injection attack
https://www.scmagazine.com/news/ai-package-hallucination-malicious-code-developer-environments
*
[Fake AI startups]
[Deepfakes]
[Sextortion]
[Undressing apps]

### AI surveillance and weaponization

Ever since it became available, the AI tech has been used by various governments to enhance their surveillance capabilities.   Recognizing faces video, automatically tracking people and objects across feeds from multiple cameras, analyzing internet traffic – now states are able to penetrate our private lives like they could never before. In fact, AI had created nothing short of a [revolution](https://www.npr.org/2023/06/13/1181868277/how-ai-is-revolutionizing-how-governments-conduct-surveillance) in how governments conduct surveillance. Perhaps in George Orwell's time, the world he described, and extent to which the government was spying on its citizens seemed grotesque, with this tech, even the most nightmarish anti-utopian scenarios become possible. Many states across the globe, such as [China](https://www.reuters.com/world/china/china-uses-ai-software-improve-its-surveillance-capabilities-2022-04-08/) and [Russia](https://www.wired.com/story/moscow-safe-city-ntechlab/) are already using AI to expand and enhance their control over their populations. For example, the system created by Moscow’s government, dubbed Safe City, was touted by city officials as a way to streamline its public safety systems. In recent years, however, its 217,000 surveillance cameras, designed to catch criminals and terrorists, have been turned against protestors, political rivals, and journalists. Utilizing one of the most advanced facial recognition systems – Sfera – Russian government was able to take totalitarian control to a completely new level. All in all, AI use in surveillance poses unprecedented danger to civil liberties around the world. In fact, [the vast majority of computer vision research leads to technology that surveils human beings.](https://www.404media.co/how-the-surveillance-ai-pipeline-literally-objectifies-human-beings/) Signal's CEO Meredith Whittaker goes as far as calling [AI a fundamentally "surveillance technology."](https://techcrunch.com/2023/09/25/signals-meredith-whittaker-ai-is-fundamentally-a-surveillance-technology)

It's not hard to imagine what kind of impact a tech so powerful could have on military industry. [Forbes reports](https://www.forbes.com/sites/cognitiveworld/2019/01/14/the-weaponization-of-artificial-intelligence/?sh=6f7ce9243686) that "The reality today is that artificial intelligence is leading us toward a new algorithmic warfare battlefield that has no boundaries or borders, may or may not have humans involved, and will be impossible to understand and perhaps control across the human ecosystem in cyberspace, geospace, and space (CGS)." Very little is known about the weapons that are being developed with the use of AI as this kind of research is rarely made open to public. However, I think it's fair to say that this kind of weaponry has the potential to be immensely dangerous. One of the example of AI weapons is Autonomous Weapon Systems (AWS). As  Birgitta Dresp-Langley, an AI researcher [writes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10030838/): "The potential consequences of a deployment of AWS for citizen stakeholders are incommensurable..." 
*
Israel AI weapons
*

### AI is hurting our planet

Even if it isn't [as bad as crypto](https://www.smithsonianmag.com/smart-news/bitcoin-could-rival-beef-or-crude-oil-in-environmental-impact-180980877/), the negative impact on climate from running AI models is [still significant.](https://www.forbes.com/sites/bernardmarr/2023/03/22/green-intelligence-why-data-and-ai-must-become-more-sustainable/?sh=52ba73d87658) The MIT Technology Review reported that training just one AI model can emit more than 626,00 pounds of carbon dioxide equivalent – which is nearly five times the lifetime emissions of an average American car.
*
AI training consuming water during draught
*

### Wrapping up

This list isn't exhaustive, there're many other problems, and many more might probably manifest in the future. However, it's worthy of note that all of the issues in this list are very significant and should be addressed as quickly as possible. Considering this, they sure must be at the center of the conversations that surround the AI, right? Leading experts, top scientists, government representatives, and millionaires, everyone should be talking about these problems whenever they talk about AI. After all, this is the _responsible_ thing to do. So let's take a look at what the average article about AI looks like. Here's a piece from NPR, a very respectable news outlet. They report that... [leading experts warn of a risk of extinction from AI?..](https://www.npr.org/2023/05/30/1178943163/ai-risk-extinction-chatgpt) Ok a bit weird way to put it, but I guess what they mean is that if you lose your job due to being replaced by an AI, and if you don't have any other means of subsistence your life could be at risk. Right? Let's take a closer look. The article reports: "AI experts issued a dire warning on Tuesday: Artificial intelligence models could soon be smarter and more powerful than us and it is time to impose limits to ensure they don't take control over humans or destroy the world." Wait, what? There're many very significant issues with this technology, many of which I have listed above, but none of them include "taking control over humans" or "destroying the world". The latter threat might be closer to truth, but the problem is that when people say "AI will destroy the world", they aren't talking about real world scenarios such as climate impact, they are talking about a completely made up plots where AI is acting like a cartoon super-villain. It seems that at least half of all AI-related news articles are about AI destroying the world, or enslaving humanity, or achieving "singularity" or some other silly sci-fi nonsense that has nothing to do with real life. How could this happen? In the next part, we will dissect some of the lies spread by AI-doomsayers and AI-utopists, and will try to figure out the motivation behind their actions. Also, by the end of this post I'm hoping to establish that AI isn't that powerful or even all that intelligent so the talks of it potentially enslaving humanity or blowing up our planet are pretty much baseless.

[Paragraph about Geoffrey Hinton]

So what can I add to all this? I haven’t lost all my faith in humanity just yet, but man is this culture around AI just stupid.

I knew that Apple Vision Pro was going to flop. but I kept my incredible predictions to myself, because I didn’t want to be wrong. But what’s the lesson here? I guess, the lesson is that I should no longer keep my very precise and very correct valuable predictions hidden from the world anymore. Because you can’t even imagine how tired I am... I’m tire from being right all the time, again and again. 

[Acknowledgements and recommendations]

